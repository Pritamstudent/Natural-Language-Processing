{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a15b3b",
   "metadata": {},
   "source": [
    "## import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7273b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "file = open(\"sample.txt\", 'r')\n",
    "text = ''\n",
    "for i in file.readlines():\n",
    "    text+=i\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841ca02",
   "metadata": {},
   "source": [
    "### 1) Trim unwanted spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87522bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_text = text.strip()\n",
    "print(trimmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ffc3a",
   "metadata": {},
   "source": [
    "### 2) Convert to lower or upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8055ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"If we are trying to understand \n",
    "the sentiment of a given tweet, we \n",
    "may not convert the tweet to lowercase \n",
    "because uppercase is often used to emphasize sentiment i.e. \n",
    "'AWESOME'and 'awesome' have different levels of emphasis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea02d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_text = trimmed_text.lower()\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e7cfa",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize the text and determine its vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d965cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize using the word_tokenizer which handles\n",
    "# the smileys, hashtags and etc. ....\n",
    "\n",
    "tokenized_list = word_tokenize(converted_text)\n",
    "print(len(tokenized_list))\n",
    "print(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c14cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization using word punct tokenizer\n",
    "punct_token_list = wordpunct_tokenize(converted_text)\n",
    "print(len(punct_token_list))\n",
    "print((punct_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03593192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocab\n",
    "vocab_set = set(tokenized_list)\n",
    "print(len(vocab_set))\n",
    "print((vocab_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2d816",
   "metadata": {},
   "source": [
    "### Step 4: Remove stop words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25621ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_wo_stopwords = vocab_set - set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb09e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18802956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "set_wo_punct = set_wo_stopwords - set(punctuation)\n",
    "print(set_wo_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950109a",
   "metadata": {},
   "source": [
    "### Step 6: Normalize the text using stemming and/or lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47c46e",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfb6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_list = []\n",
    "stemObj = SnowballStemmer(\"english\")\n",
    "for i in set_wo_punct:\n",
    "    stemmed_list.append(stemObj.stem(i))\n",
    "    \n",
    "print(stemmed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c2b78",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parts of speech\n",
    "pos_tag_list = pos_tag(set_wo_punct)\n",
    "print(pos_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe483044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for getting the parts of speech\n",
    "from nltk import wordnet\n",
    "\n",
    "#Lemmatization\n",
    "lemma_list = []\n",
    "lemmaObj = WordNetLemmatizer()\n",
    "for word, pos in pos_tag_list:\n",
    "    if pos.startswith('J'):  # Adjective\n",
    "        lemma = lemmaObj.lemmatize(word, 'a')\n",
    "    elif pos.startswith('V'):  # Verb\n",
    "        lemma = lemmaObj.lemmatize(word, 'v')\n",
    "    elif pos.startswith('N'):  # Noun\n",
    "        lemma = lemmaObj.lemmatize(word, 'n')\n",
    "    elif pos.startswith('R'):  # Adverb\n",
    "        lemma = lemmaObj.lemmatize(word, 'r')\n",
    "    else:\n",
    "        lemma = word\n",
    "    lemma_list.append(lemma)\n",
    "        \n",
    "print(lemma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162d022",
   "metadata": {},
   "source": [
    "### Step 7: Create n-grams from text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf22cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = ngrams(set_wo_punct, 2)\n",
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e517a12",
   "metadata": {},
   "source": [
    "## Regular Expressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7050c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e6c7a",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199323e3",
   "metadata": {},
   "source": [
    "Search method searches for the string present in the r\"\" in the whole input string and returns a match object if there is a match else returns None. Search only returns the first match present in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e29768",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= \"1947 was when India got their independence. \"\n",
    "print(\" Occurances of a-z: \", re.search(r\"[a-z]+\", sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a10cd",
   "metadata": {},
   "source": [
    "The match at the string \"1947\" is the only string according to search method because our set only ranges from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"1947 was when India became independent.\"\n",
    "print(\" Occurances of 0-9: \", re.search(r\"[0-9]+\", sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af344399",
   "metadata": {},
   "source": [
    "Our desired set is namely a-z, A-Z, 0-9, '_' and a space character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1868c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"1947_was when India became independent.\"\n",
    "print(\"Occurances of w and space: \", re.search(r\"[\\w ]+\", sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b5839",
   "metadata": {},
   "source": [
    "# substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910302c",
   "metadata": {},
   "source": [
    "Sub is substitution of a substring with another string in the given input string. So understandably it takes three parameters. \n",
    "\n",
    "First argument is the string to be removed, second argument is the resultant string and the last argument is the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"I like coffee\" \n",
    "print(re.sub(r\"coffee\",\"tea\",sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9867a",
   "metadata": {},
   "source": [
    "## Findall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2753fc3",
   "metadata": {},
   "source": [
    "Findall parses our input string from left to right and returns all the substrings matching with our raw string as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e18578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(re.findall(r\"coffee\",sent))\n",
    "\n",
    "print(len(re.findall(r\"coffee\",sent)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10583ee3",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1755a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "barack = \"\"\"Barack Hussein Obama (born August 4, 1961) is an American politician \n",
    "who served as the 44th President of the United States from January 20, 2009, to January 20, 2017.\n",
    "A member of the Democratic Party, he was the first African American to assume the presidency \n",
    "and previously served as a United States Senator from Illinois (2005–2008).\"\"\"\n",
    "\n",
    "token_barack = word_tokenize(barack)\n",
    "pos_list = pos_tag(token_barack)\n",
    "print(ne_chunk(pos_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a36a65",
   "metadata": {},
   "source": [
    "The below code demonstrates the usage of RegexpParser which can give a more desirable result in comparison to the default NE Chunker. Here we need to configure how entities are determined, i.e. what kind of POS combinations results in a specific named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f7986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "grammar = r\"\"\"Place: {<NNP><NNPS>+}\n",
    "           Date: {<NNP><CD><,><CD>}\n",
    "           Person: {<NNP>+}\"\"\"\n",
    "regParser = RegexpParser(grammar)\n",
    "reg_lines = regParser.parse(pos_list)\n",
    "print(reg_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08254faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = \"\"\"Donald John Trump (born June 14, 1946) is the 45th and current President of the United States.\n",
    "Before entering politics, he was a businessman and television personality. \n",
    "Trump was born and raised in the New York City borough of Queens, and received an economics degree from the\n",
    " Wharton School of the University of Pennsylvania. \n",
    "He took charge of his family's real estate business in 1971, renamed it The Trump Organization, and expanded \n",
    "it from Queens and Brooklyn into Manhattan. \n",
    "The company built or renovated skyscrapers, hotels, casinos, and golf courses. \n",
    "Trump later started various side ventures, including licensing his name for real estate and consumer products.\n",
    "He managed the company until his 2017 inauguration. \n",
    "He co-authored several books, including The Art of the Deal. He owned the Miss Universe and Miss USA beauty \n",
    "pageants from 1996 to 2015, and he produced and hosted the reality television show The Apprentice from 2003 to 2015.\n",
    "Forbes estimates his net worth to be $3.1 billion.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44045953",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \" The race offcials refused to permit the team to race roday\"\n",
    "print(pos_tag(word_tokenize(sent1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4bc7a9",
   "metadata": {},
   "source": [
    "Here, both the races are classified as NOUN which is incorrect. Hence, it is making the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfade536",
   "metadata": {},
   "source": [
    "## Default Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "#brown is a corpus\n",
    "\n",
    "# Trying to get the most common tag in the brown corpus\n",
    "\n",
    "tags = [tag for (word,tag) in brown.tagged_words()]\n",
    "most_common_tag = nltk.FreqDist(tags).max()\n",
    "print(most_common_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the most common tagger as the input for the default tagger\n",
    "from nltk import DefaultTagger\n",
    "default_tag = DefaultTagger(most_common_tag)\n",
    "def_tagged_barack = default_tag.tag(token_barack)\n",
    "print(def_tagged_barack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754697d1",
   "metadata": {},
   "source": [
    "## Lookup taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6800691",
   "metadata": {},
   "source": [
    "A NgramTagger tags a word based on the previous n words occurring in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"the quick brown fox jumps over the lazy dog\"\n",
    "training = pos_tag(word_tokenize(sent1))\n",
    "print(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train the n gram tagger\n",
    "ngram_tagger = nltk.NgramTagger(n=2, train = [training])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = \"the lazy dog was jumped over by the quick brown fox\"\n",
    "tag = ngram_tagger.tag(word_tokenize(sent2))\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c867c",
   "metadata": {},
   "source": [
    "This looking up of occurrence of words in the sequence appearing in the training set can be considered as the context.\n",
    "\n",
    "Therefore, we can now understand that a NgramTagger tags words that appear in context, and the context is defined by the window 'n' which is the number of tokens to consider together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5279821",
   "metadata": {},
   "source": [
    "## Example of tagging pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323fb08",
   "metadata": {},
   "source": [
    "### <b> UnigramTagger --> RegexpTagger --> DefaultTagger. Note that --> indicates backoff.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44cb72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_tag = DefaultTagger('NN')\n",
    "patterns = [\n",
    "    (r'.*\\'s$', 'NN$'), #possessive nouns\n",
    "    (r'.*es$','VBZ'),\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "    (r'[Aa][Nn][Dd]','CC'),\n",
    "    (r'.*ed$', 'VBD'),\n",
    "    (r',' , ','),\n",
    "    (r'.*ould$', 'MD'),\n",
    "    (r'.*ing$', 'VBG'),\n",
    "    (r'.*s$', 'NNS'),\n",
    "]\n",
    "regexp_tag = nltk.RegexpTagger(patterns, backoff = default_tag)\n",
    "unigram_tag = nltk.UnigramTagger(train = [pos_list], backoff = regexp_tag)\n",
    "trump_tag = unigram_tag.tag(word_tokenize(trump))\n",
    "print(trump_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5117f",
   "metadata": {},
   "source": [
    "### In the above code, the UnigramTagger is first invoked to tag the tokens in the Trump article. Whichever words are tagged None by this UnigramTagger are then sent as backoff to the RegexpTagger. The RegexpTagger then tags the words based on the patterns rule it is fed. Any words that are still left untagged are then sent to the DefaultTagger as backoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74d0bd",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6a19c",
   "metadata": {},
   "source": [
    "Let us now deal with the collection of the documents .\n",
    "Collection of the documents is called <b> Corpus <b> . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ddfcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown \n",
    "\n",
    "# brown corpus is a tagged corpus where each word in each file of the corpus is associated \n",
    "# with the POS tag\n",
    "\n",
    "# Display all the files in the corpus\n",
    "print(brown.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2140d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the contents of the file\n",
    "# in the no. of paragraphs, sentences and words\n",
    "print(brown.paras('ck11'))\n",
    "print(\"----------------------------------------\")\n",
    "print(brown.sents('ck11'))\n",
    "print(\"----------------------------------------\")\n",
    "print(brown.words('ck11'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737da17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display yhe POS tag for each word in a specific file of the corpus\n",
    "print(brown.tagged_words('ck11'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0304f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "print(conll2000.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e1605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conll2000.words('train.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conll2000.chunked_sents('train.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db3e9e",
   "metadata": {},
   "source": [
    "1. What are the different types of Chunking in NLP?\n",
    "Group of words make up phrases and there are five major categories.\n",
    "  - Noun Phrase (NP)\n",
    "  - Verb phrase (VP)\n",
    "  - Adjective phrase (ADJP)\n",
    "  - Adverb phrase (ADVP)\n",
    "  - Prepositional phrase (PP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c5ddd",
   "metadata": {},
   "source": [
    "# Make our own corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea189a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "path =  \"E:/Natural Language Processing/text_doc/\"\n",
    "president_corpus = PlaintextCorpusReader(path, \".*\")\n",
    "print(president_corpus.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c3f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the senetences in a sepcifc file\n",
    "print((president_corpus.sents('barack.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the sentences in all the files of the corpus\n",
    "print(president_corpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c893854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the words in as specific file\n",
    "print(president_corpus.words('barack.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(president_corpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743ef8d",
   "metadata": {},
   "source": [
    "Note that the PlainTextCorpusReader automatically splits the text data into paragraphs, sentences and words using appropriate tokenizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c2d9f",
   "metadata": {},
   "source": [
    "# Vectorizing textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "sen1= \"India is a republic country. We are proud Indians.\"\n",
    "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
    "count_vectorizer = CountVectorizer()\n",
    "# DTM = document text matrix\n",
    "dtm = count_vectorizer.fit_transform([sent1,sent2])\n",
    "print(type(dtm))\n",
    "print(type(dtm.toarray()))\n",
    "print(pd.DataFrame(data = dtm.toarray(), columns = count_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e0d9b",
   "metadata": {},
   "source": [
    "The similarity between the two documents can now be found using commonly used distance metrics like Euclidean distance or cosine distance.\n",
    "\n",
    "The below code demonstrates the use of cosine distance. The smaller this value, the more similar the two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0acc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "print(cosine(dtm[0].toarray().flat, dtm[1].toarray().flat))\n",
    "# cosine only accepts 1D vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f4702",
   "metadata": {},
   "source": [
    "The values is very high so the two documents are not very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112eccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "sent1 = \"India is a republic country. We are proud Indians.\"\n",
    "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform([sent1,sent2])\n",
    "print(pd.DataFrame(data = tfidf_vectors.toarray(), columns = tfidf_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us calculate the cosine distance\n",
    "from scipy.spatial.distance import cosine\n",
    "print(cosine(tfidf_vectors[0].toarray().flat, tfidf_vectors[1].toarray().flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd455216",
   "metadata": {},
   "source": [
    "Since the values is high, they are not similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let us now determine the Tf-IDF vectors and similarities between the 3 documents\n",
    "#  in our corpus (president_corpus) \n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "\n",
    "path = \"E:/Natural Language Processing/text_doc/\"\n",
    "president_corpus = PlaintextCorpusReader(path, \".*\", encoding = \"utf-8\")\n",
    "\n",
    "tf_idf = TfidfVectorizer(input='filename')\n",
    "files = [path+filename for filename in list(president_corpus.fileids())]\n",
    "tf_idf_matrix = tf_idf.fit_transform(raw_documents = files)\n",
    "print(president_corpus.fileids())\n",
    "barack = tf_idf_matrix.toarray()[0]\n",
    "bush = tf_idf_matrix.toarray()[1]\n",
    "trump = tf_idf_matrix.toarray()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaf082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine distance\n",
    "from scipy.spatial.distance import cosine\n",
    "print(\"Distance between articles on barack and bush is: \",cosine(barack, bush))\n",
    "\n",
    "print(\"Distance between articles on bush and trump is: \", cosine(bush, trump))\n",
    "\n",
    "print(\"Distance between articles on barack and trump is: \", cosine(barack, trump))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c87e3",
   "metadata": {},
   "source": [
    "The most related documents are bush and trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a59514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "def euclidean_distance(x,y):\n",
    "    return sqrt(sum(pow(a-b,2) for a , b in zip(x, y)))\n",
    "print(\"Distance between articles on barack and trump is: \", euclidean_distance(barack, trump))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f11869",
   "metadata": {},
   "source": [
    "# Find about jaccard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1ca29",
   "metadata": {},
   "source": [
    "Jaccard Similarity is used to find similarities between sets. The Jaccard similarity measures similarity between finite sample sets and is defined as the cardinality of the intersection of sets divided by the cardinality of the union of the sample sets.\n",
    "Suppose you want to find Jaccard similarity between two sets A and B, it is the ratio of the cardinality of A ∩ B and A ∪ B.\n",
    "Jaccard Similarity J(A,B) = |A∩B|/|A∪B|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdcd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "def jaccard(x, y):\n",
    "    \n",
    "    intersect_card = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_card = len(set.union(*[set(x), set(y)]))\n",
    "    return intersect_card/float(union_card)\n",
    "\n",
    "print(\"Distance between articles on barack and trump is: \", jaccard(barack, trump))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fcbad",
   "metadata": {},
   "source": [
    "If two datasets share the exact same members, their Jaccard Similarity Index will be 1. Conversely, if they have no members in common then their similarity will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607ff84",
   "metadata": {},
   "source": [
    "# Wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a2dff",
   "metadata": {},
   "source": [
    "## detect unusual words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c37969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "sent1 = \"\"\"Just forced myself to eat a slice. I'm really not hungry tho. \n",
    "           Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\"\"\"\n",
    "sent2 = \"I call you later, don't have nw. If urgnt, sms me.\"\n",
    "sent3 = \"Watching a telugu movie..wat abt u?\"\n",
    "def find_unusual_words(text):\n",
    "    \n",
    "    # finds the vocab set for the words in the sentecnce \n",
    "    # and convert them to lowercase if they are alphabets\n",
    "    text_vocab_set = set(w.lower() for w in text if w.isalpha())\n",
    "    \n",
    "    # make the vocabulary set for the corpus\n",
    "    english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual_set = text_vocab_set - english_vocab_set\n",
    "    return sorted(unusual_set)\n",
    "\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent1)))\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent2)))\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d14fc",
   "metadata": {},
   "source": [
    "## Detect Possible Mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc18c5",
   "metadata": {},
   "source": [
    "If a word could not found in the word list, it is probable that it is a spelling mistake.\n",
    "\n",
    "The below code, compares the unusual words with known words and suggests possible words based on edit distance. Edit distance is the measure of how similar or dissimilar two words are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df99ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unusual_words_found = ['knows', 'lol', 'nw', 'sms', 'urgnt', 'abt']\n",
    "from nltk.metrics import edit_distance\n",
    "possible_suggestions = {}\n",
    "english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "for unusual_word in unusual_words_found:\n",
    "    \n",
    "    for word in english_vocab_set :\n",
    "        \n",
    "        dist = edit_distance(unusual_word, word)\n",
    "        if dist < len(unusual_word)/2:\n",
    "            \n",
    "            if unusual_word not in possible_suggestions.keys():\n",
    "                \n",
    "                possible_suggestions[unusual_word] = [word]\n",
    "            else:\n",
    "                \n",
    "                possible_suggestions[unusual_word].append(word)\n",
    "\n",
    "print(possible_suggestions[\"lol\"])\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917b8fe",
   "metadata": {},
   "source": [
    "# Detect the names of the people "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f90c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('names')\n",
    "def names_in_text(text):\n",
    "    \n",
    "    names = []\n",
    "    words_set = set(i for i in text if i.isalpha())\n",
    "    male_names = nltk.corpus.names.words('male.txt')\n",
    "    female_names = nltk.corpus.names.words('female.txt')\n",
    "    for w in words_set:\n",
    "        if male_names.count(w) > 0 or female_names.count(w) > 0:\n",
    "            \n",
    "            names.append(w)\n",
    "\n",
    "    return names\n",
    "\n",
    "sent1 = \"John and Mary go to the church every Sunday\"\n",
    "sent2 = \"No man has ever seen the dark side of the Moon\"\n",
    "print(names_in_text(word_tokenize(sent1)))\n",
    "print(names_in_text(word_tokenize(sent2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8197a775",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e01316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# To get the all possible meanings of the word \"dog\"\n",
    "print(wn.synsets(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029230e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the all lemma names of \"dog\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
