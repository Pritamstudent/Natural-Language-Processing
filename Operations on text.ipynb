{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a15b3b",
   "metadata": {},
   "source": [
    "## import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7273b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bfeb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history of natural language processing generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n",
      "\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n",
      "\n",
      "Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n",
      "\n",
      "During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n",
      "\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n"
     ]
    }
   ],
   "source": [
    "# Read the file\n",
    "file = open(\"sample.txt\", 'r')\n",
    "text = ''\n",
    "for i in file.readlines():\n",
    "    text+=i\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841ca02",
   "metadata": {},
   "source": [
    "### 1) Trim unwanted spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87522bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history of natural language processing generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n",
      "\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n",
      "\n",
      "Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n",
      "\n",
      "During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n",
      "\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n"
     ]
    }
   ],
   "source": [
    "trimmed_text = text.strip()\n",
    "print(trimmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ffc3a",
   "metadata": {},
   "source": [
    "### 2) Convert to lower or upper case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8055ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If we are trying to understand \\nthe sentiment of a given tweet, we \\nmay not convert the tweet to lowercase \\nbecause uppercase is often used to emphasize sentiment i.e. \\n'AWESOME'and 'awesome' have different levels of emphasis.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"If we are trying to understand \n",
    "the sentiment of a given tweet, we \n",
    "may not convert the tweet to lowercase \n",
    "because uppercase is often used to emphasize sentiment i.e. \n",
    "'AWESOME'and 'awesome' have different levels of emphasis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea02d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the history of natural language processing generally started in the 1950s, although work can be found from earlier periods. in 1950, alan turing published an article titled \"intelligence\" which proposed what is now called the turing test as a criterion of intelligence.\n",
      "\n",
      "the georgetown experiment in 1954 involved fully automatic translation of more than sixty russian sentences into english. the authors claimed that within three or five years, machine translation would be a solved problem.[2] however, real progress was much slower, and after the alpac report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n",
      "\n",
      "some notably successful natural language processing systems developed in the 1960s were shrdlu, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and eliza, a simulation of a rogerian psychotherapist, written by joseph weizenbaum between 1964 and 1966. using almost no information about human thought or emotion, eliza sometimes provided a startlingly human-like interaction. when the \"patient\" exceeded the very small knowledge base, eliza might provide a generic response, for example, responding to \"my head hurts\" with \"why do you say your head hurts?\".\n",
      "\n",
      "during the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. examples are margie (schank, 1975), sam (cullingford, 1978), pam (wilensky, 1978), talespin (meehan, 1976), qualm (lehnert, 1977), politics (carbonell, 1979), and plot units (lehnert 1981). during this time, many chatterbots were written including parry, racter, and jabberwacky.\n",
      "\n",
      "up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. this was due to both the steady increase in computational power (see moore's law) and the gradual lessening of the dominance of chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. however, part-of-speech tagging introduced the use of hidden markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. the cache language models upon which many speech recognition systems now rely are examples of such statistical models. such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n"
     ]
    }
   ],
   "source": [
    "converted_text = trimmed_text.lower()\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e7cfa",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize the text and determine its vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d965cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576\n",
      "['the', 'history', 'of', 'natural', 'language', 'processing', 'generally', 'started', 'in', 'the', '1950s', ',', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', '.', 'in', '1950', ',', 'alan', 'turing', 'published', 'an', 'article', 'titled', '``', 'intelligence', \"''\", 'which', 'proposed', 'what', 'is', 'now', 'called', 'the', 'turing', 'test', 'as', 'a', 'criterion', 'of', 'intelligence', '.', 'the', 'georgetown', 'experiment', 'in', '1954', 'involved', 'fully', 'automatic', 'translation', 'of', 'more', 'than', 'sixty', 'russian', 'sentences', 'into', 'english', '.', 'the', 'authors', 'claimed', 'that', 'within', 'three', 'or', 'five', 'years', ',', 'machine', 'translation', 'would', 'be', 'a', 'solved', 'problem', '.', '[', '2', ']', 'however', ',', 'real', 'progress', 'was', 'much', 'slower', ',', 'and', 'after', 'the', 'alpac', 'report', 'in', '1966', ',', 'which', 'found', 'that', 'ten-year-long', 'research', 'had', 'failed', 'to', 'fulfill', 'the', 'expectations', ',', 'funding', 'for', 'machine', 'translation', 'was', 'dramatically', 'reduced', '.', 'little', 'further', 'research', 'in', 'machine', 'translation', 'was', 'conducted', 'until', 'the', 'late', '1980s', ',', 'when', 'the', 'first', 'statistical', 'machine', 'translation', 'systems', 'were', 'developed', '.', 'some', 'notably', 'successful', 'natural', 'language', 'processing', 'systems', 'developed', 'in', 'the', '1960s', 'were', 'shrdlu', ',', 'a', 'natural', 'language', 'system', 'working', 'in', 'restricted', '``', 'blocks', 'worlds', \"''\", 'with', 'restricted', 'vocabularies', ',', 'and', 'eliza', ',', 'a', 'simulation', 'of', 'a', 'rogerian', 'psychotherapist', ',', 'written', 'by', 'joseph', 'weizenbaum', 'between', '1964', 'and', '1966.', 'using', 'almost', 'no', 'information', 'about', 'human', 'thought', 'or', 'emotion', ',', 'eliza', 'sometimes', 'provided', 'a', 'startlingly', 'human-like', 'interaction', '.', 'when', 'the', '``', 'patient', \"''\", 'exceeded', 'the', 'very', 'small', 'knowledge', 'base', ',', 'eliza', 'might', 'provide', 'a', 'generic', 'response', ',', 'for', 'example', ',', 'responding', 'to', '``', 'my', 'head', 'hurts', \"''\", 'with', '``', 'why', 'do', 'you', 'say', 'your', 'head', 'hurts', '?', \"''\", '.', 'during', 'the', '1970s', ',', 'many', 'programmers', 'began', 'to', 'write', '``', 'conceptual', 'ontologies', \"''\", ',', 'which', 'structured', 'real-world', 'information', 'into', 'computer-understandable', 'data', '.', 'examples', 'are', 'margie', '(', 'schank', ',', '1975', ')', ',', 'sam', '(', 'cullingford', ',', '1978', ')', ',', 'pam', '(', 'wilensky', ',', '1978', ')', ',', 'talespin', '(', 'meehan', ',', '1976', ')', ',', 'qualm', '(', 'lehnert', ',', '1977', ')', ',', 'politics', '(', 'carbonell', ',', '1979', ')', ',', 'and', 'plot', 'units', '(', 'lehnert', '1981', ')', '.', 'during', 'this', 'time', ',', 'many', 'chatterbots', 'were', 'written', 'including', 'parry', ',', 'racter', ',', 'and', 'jabberwacky', '.', 'up', 'to', 'the', '1980s', ',', 'most', 'natural', 'language', 'processing', 'systems', 'were', 'based', 'on', 'complex', 'sets', 'of', 'hand-written', 'rules', '.', 'starting', 'in', 'the', 'late', '1980s', ',', 'however', ',', 'there', 'was', 'a', 'revolution', 'in', 'natural', 'language', 'processing', 'with', 'the', 'introduction', 'of', 'machine', 'learning', 'algorithms', 'for', 'language', 'processing', '.', 'this', 'was', 'due', 'to', 'both', 'the', 'steady', 'increase', 'in', 'computational', 'power', '(', 'see', 'moore', \"'s\", 'law', ')', 'and', 'the', 'gradual', 'lessening', 'of', 'the', 'dominance', 'of', 'chomskyan', 'theories', 'of', 'linguistics', '(', 'e.g', '.', 'transformational', 'grammar', ')', ',', 'whose', 'theoretical', 'underpinnings', 'discouraged', 'the', 'sort', 'of', 'corpus', 'linguistics', 'that', 'underlies', 'the', 'machine-learning', 'approach', 'to', 'language', 'processing', '.', '[', '3', ']', 'some', 'of', 'the', 'earliest-used', 'machine', 'learning', 'algorithms', ',', 'such', 'as', 'decision', 'trees', ',', 'produced', 'systems', 'of', 'hard', 'if-then', 'rules', 'similar', 'to', 'existing', 'hand-written', 'rules', '.', 'however', ',', 'part-of-speech', 'tagging', 'introduced', 'the', 'use', 'of', 'hidden', 'markov', 'models', 'to', 'natural', 'language', 'processing', ',', 'and', 'increasingly', ',', 'research', 'has', 'focused', 'on', 'statistical', 'models', ',', 'which', 'make', 'soft', ',', 'probabilistic', 'decisions', 'based', 'on', 'attaching', 'real-valued', 'weights', 'to', 'the', 'features', 'making', 'up', 'the', 'input', 'data', '.', 'the', 'cache', 'language', 'models', 'upon', 'which', 'many', 'speech', 'recognition', 'systems', 'now', 'rely', 'are', 'examples', 'of', 'such', 'statistical', 'models', '.', 'such', 'models', 'are', 'generally', 'more', 'robust', 'when', 'given', 'unfamiliar', 'input', ',', 'especially', 'input', 'that', 'contains', 'errors', '(', 'as', 'is', 'very', 'common', 'for', 'real-world', 'data', ')', ',', 'and', 'produce', 'more', 'reliable', 'results', 'when', 'integrated', 'into', 'a', 'larger', 'system', 'comprising', 'multiple', 'subtasks', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize using the word_tokenizer which handles\n",
    "# the smileys, hashtags and etc. ....\n",
    "\n",
    "tokenized_list = word_tokenize(converted_text)\n",
    "print(len(tokenized_list))\n",
    "print(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c14cb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "['the', 'history', 'of', 'natural', 'language', 'processing', 'generally', 'started', 'in', 'the', '1950s', ',', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', '.', 'in', '1950', ',', 'alan', 'turing', 'published', 'an', 'article', 'titled', '\"', 'intelligence', '\"', 'which', 'proposed', 'what', 'is', 'now', 'called', 'the', 'turing', 'test', 'as', 'a', 'criterion', 'of', 'intelligence', '.', 'the', 'georgetown', 'experiment', 'in', '1954', 'involved', 'fully', 'automatic', 'translation', 'of', 'more', 'than', 'sixty', 'russian', 'sentences', 'into', 'english', '.', 'the', 'authors', 'claimed', 'that', 'within', 'three', 'or', 'five', 'years', ',', 'machine', 'translation', 'would', 'be', 'a', 'solved', 'problem', '.[', '2', ']', 'however', ',', 'real', 'progress', 'was', 'much', 'slower', ',', 'and', 'after', 'the', 'alpac', 'report', 'in', '1966', ',', 'which', 'found', 'that', 'ten', '-', 'year', '-', 'long', 'research', 'had', 'failed', 'to', 'fulfill', 'the', 'expectations', ',', 'funding', 'for', 'machine', 'translation', 'was', 'dramatically', 'reduced', '.', 'little', 'further', 'research', 'in', 'machine', 'translation', 'was', 'conducted', 'until', 'the', 'late', '1980s', ',', 'when', 'the', 'first', 'statistical', 'machine', 'translation', 'systems', 'were', 'developed', '.', 'some', 'notably', 'successful', 'natural', 'language', 'processing', 'systems', 'developed', 'in', 'the', '1960s', 'were', 'shrdlu', ',', 'a', 'natural', 'language', 'system', 'working', 'in', 'restricted', '\"', 'blocks', 'worlds', '\"', 'with', 'restricted', 'vocabularies', ',', 'and', 'eliza', ',', 'a', 'simulation', 'of', 'a', 'rogerian', 'psychotherapist', ',', 'written', 'by', 'joseph', 'weizenbaum', 'between', '1964', 'and', '1966', '.', 'using', 'almost', 'no', 'information', 'about', 'human', 'thought', 'or', 'emotion', ',', 'eliza', 'sometimes', 'provided', 'a', 'startlingly', 'human', '-', 'like', 'interaction', '.', 'when', 'the', '\"', 'patient', '\"', 'exceeded', 'the', 'very', 'small', 'knowledge', 'base', ',', 'eliza', 'might', 'provide', 'a', 'generic', 'response', ',', 'for', 'example', ',', 'responding', 'to', '\"', 'my', 'head', 'hurts', '\"', 'with', '\"', 'why', 'do', 'you', 'say', 'your', 'head', 'hurts', '?\".', 'during', 'the', '1970s', ',', 'many', 'programmers', 'began', 'to', 'write', '\"', 'conceptual', 'ontologies', '\",', 'which', 'structured', 'real', '-', 'world', 'information', 'into', 'computer', '-', 'understandable', 'data', '.', 'examples', 'are', 'margie', '(', 'schank', ',', '1975', '),', 'sam', '(', 'cullingford', ',', '1978', '),', 'pam', '(', 'wilensky', ',', '1978', '),', 'talespin', '(', 'meehan', ',', '1976', '),', 'qualm', '(', 'lehnert', ',', '1977', '),', 'politics', '(', 'carbonell', ',', '1979', '),', 'and', 'plot', 'units', '(', 'lehnert', '1981', ').', 'during', 'this', 'time', ',', 'many', 'chatterbots', 'were', 'written', 'including', 'parry', ',', 'racter', ',', 'and', 'jabberwacky', '.', 'up', 'to', 'the', '1980s', ',', 'most', 'natural', 'language', 'processing', 'systems', 'were', 'based', 'on', 'complex', 'sets', 'of', 'hand', '-', 'written', 'rules', '.', 'starting', 'in', 'the', 'late', '1980s', ',', 'however', ',', 'there', 'was', 'a', 'revolution', 'in', 'natural', 'language', 'processing', 'with', 'the', 'introduction', 'of', 'machine', 'learning', 'algorithms', 'for', 'language', 'processing', '.', 'this', 'was', 'due', 'to', 'both', 'the', 'steady', 'increase', 'in', 'computational', 'power', '(', 'see', 'moore', \"'\", 's', 'law', ')', 'and', 'the', 'gradual', 'lessening', 'of', 'the', 'dominance', 'of', 'chomskyan', 'theories', 'of', 'linguistics', '(', 'e', '.', 'g', '.', 'transformational', 'grammar', '),', 'whose', 'theoretical', 'underpinnings', 'discouraged', 'the', 'sort', 'of', 'corpus', 'linguistics', 'that', 'underlies', 'the', 'machine', '-', 'learning', 'approach', 'to', 'language', 'processing', '.[', '3', ']', 'some', 'of', 'the', 'earliest', '-', 'used', 'machine', 'learning', 'algorithms', ',', 'such', 'as', 'decision', 'trees', ',', 'produced', 'systems', 'of', 'hard', 'if', '-', 'then', 'rules', 'similar', 'to', 'existing', 'hand', '-', 'written', 'rules', '.', 'however', ',', 'part', '-', 'of', '-', 'speech', 'tagging', 'introduced', 'the', 'use', 'of', 'hidden', 'markov', 'models', 'to', 'natural', 'language', 'processing', ',', 'and', 'increasingly', ',', 'research', 'has', 'focused', 'on', 'statistical', 'models', ',', 'which', 'make', 'soft', ',', 'probabilistic', 'decisions', 'based', 'on', 'attaching', 'real', '-', 'valued', 'weights', 'to', 'the', 'features', 'making', 'up', 'the', 'input', 'data', '.', 'the', 'cache', 'language', 'models', 'upon', 'which', 'many', 'speech', 'recognition', 'systems', 'now', 'rely', 'are', 'examples', 'of', 'such', 'statistical', 'models', '.', 'such', 'models', 'are', 'generally', 'more', 'robust', 'when', 'given', 'unfamiliar', 'input', ',', 'especially', 'input', 'that', 'contains', 'errors', '(', 'as', 'is', 'very', 'common', 'for', 'real', '-', 'world', 'data', '),', 'and', 'produce', 'more', 'reliable', 'results', 'when', 'integrated', 'into', 'a', 'larger', 'system', 'comprising', 'multiple', 'subtasks', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using word punct tokenizer\n",
    "punct_token_list = wordpunct_tokenize(converted_text)\n",
    "print(len(punct_token_list))\n",
    "print((punct_token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03593192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n",
      "{'claimed', ']', 'whose', 'authors', 'within', 'a', 'the', 'criterion', 'after', 'carbonell', 'called', 'units', 'decision', 'provide', 'your', 'weights', 'proposed', 'test', '1950', 'human', 'jabberwacky', 'by', 'simulation', 'time', 'and', 'data', 'you', 'intelligence', '1977', 'hand-written', 'chatterbots', 'theories', 'rely', 'larger', ',', 'systems', 'underlies', 'eliza', 'head', 'some', 'worlds', 'produce', '1964', 'do', 'comprising', 'earliest-used', 'response', 'three', 'attaching', 'psychotherapist', 'errors', 'this', 'theoretical', 'work', 'startlingly', 'results', 'lehnert', 'or', '?', 'part-of-speech', 'generic', 'published', 'processing', 'periods', '1966.', 'complex', '``', 'existing', 'to', 'racter', 'algorithms', 'real-world', 'underpinnings', '1954', 'making', 'sort', 'almost', 'schank', 'be', 'emotion', 'steady', 'input', 'until', 'no', 'integrated', '1970s', 'more', 'learning', '1980s', 'blocks', 'written', 'about', '1950s', '[', 'introduced', 'has', 'can', 'cache', 'gradual', 'alpac', 'restricted', 'had', 'during', 'my', 'of', 'base', 'such', 'example', 'programmers', 'slower', 'language', 'meehan', 'chomskyan', 'sometimes', 'there', 'georgetown', 'e.g', 'into', 'computational', 'based', 'history', 'research', 'titled', 'conducted', 'was', 'system', 'focused', 'common', 'from', 'patient', 'what', 'real', 'little', 'corpus', 'thought', 'say', 'models', 'five', 'further', 'margie', 'rogerian', 'on', 'that', 'recognition', 'discouraged', 'tagging', 'both', 'much', 'due', 'article', 'fulfill', '.', 'would', '1979', 'starting', 'russian', 'between', 'sam', 'linguistics', 'in', 'very', 'transformational', 'might', 'grammar', 'hidden', 'contains', 'plot', 'make', 'fully', 'working', 'turing', 'moore', 'human-like', '3', 'robust', 'failed', 'features', 'alan', 'see', 'small', 'sixty', 'information', 'many', 'wilensky', 'reliable', 'machine-learning', 'pam', 'experiment', 'successful', 'expectations', 'conceptual', 'however', 'similar', 'probabilistic', 'politics', 'law', '1975', 'hurts', 'ontologies', 'power', 'markov', 'for', 'as', 'upon', 'ten-year-long', '1981', '2', 'real-valued', 'an', '1960s', 'structured', 'years', 'although', 'subtasks', 'english', 'were', 'most', 'rules', 'started', 'with', 'trees', \"''\", 'use', 'decisions', 'multiple', 'using', 'reduced', '(', 'solved', 'progress', 'weizenbaum', 'which', 'if-then', 'first', 'up', 'sentences', 'notably', 'exceeded', 'funding', 'soft', 'responding', '1978', 'write', 'examples', 'translation', 'late', 'approach', 'including', 'dramatically', 'report', 'why', 'unfamiliar', 'knowledge', 'hard', 'computer-understandable', 'than', 'natural', 'statistical', 'cullingford', 'is', 'revolution', 'provided', 'introduction', 'given', 'began', 'earlier', 'talespin', 'qualm', 'now', 'dominance', 'especially', ')', 'automatic', 'developed', 'parry', 'speech', 'involved', 'joseph', 'sets', 'machine', 'increase', 'found', 'shrdlu', '1976', 'vocabularies', 'are', 'interaction', 'produced', 'lessening', '1966', \"'s\", 'increasingly', 'generally', 'when', 'problem'}\n"
     ]
    }
   ],
   "source": [
    "# get the vocab\n",
    "vocab_set = set(tokenized_list)\n",
    "print(len(vocab_set))\n",
    "print((vocab_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2d816",
   "metadata": {},
   "source": [
    "### Step 4: Remove stop words from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25621ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_wo_stopwords = vocab_set - set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bb09e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\",\n",
       " \"'s\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '.',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1954',\n",
       " '1960s',\n",
       " '1964',\n",
       " '1966',\n",
       " '1966.',\n",
       " '1970s',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980s',\n",
       " '1981',\n",
       " '2',\n",
       " '3',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " '``',\n",
       " 'alan',\n",
       " 'algorithms',\n",
       " 'almost',\n",
       " 'alpac',\n",
       " 'although',\n",
       " 'approach',\n",
       " 'article',\n",
       " 'attaching',\n",
       " 'authors',\n",
       " 'automatic',\n",
       " 'base',\n",
       " 'based',\n",
       " 'began',\n",
       " 'blocks',\n",
       " 'cache',\n",
       " 'called',\n",
       " 'carbonell',\n",
       " 'chatterbots',\n",
       " 'chomskyan',\n",
       " 'claimed',\n",
       " 'common',\n",
       " 'complex',\n",
       " 'comprising',\n",
       " 'computational',\n",
       " 'computer-understandable',\n",
       " 'conceptual',\n",
       " 'conducted',\n",
       " 'contains',\n",
       " 'corpus',\n",
       " 'criterion',\n",
       " 'cullingford',\n",
       " 'data',\n",
       " 'decision',\n",
       " 'decisions',\n",
       " 'developed',\n",
       " 'discouraged',\n",
       " 'dominance',\n",
       " 'dramatically',\n",
       " 'due',\n",
       " 'e.g',\n",
       " 'earlier',\n",
       " 'earliest-used',\n",
       " 'eliza',\n",
       " 'emotion',\n",
       " 'english',\n",
       " 'errors',\n",
       " 'especially',\n",
       " 'example',\n",
       " 'examples',\n",
       " 'exceeded',\n",
       " 'existing',\n",
       " 'expectations',\n",
       " 'experiment',\n",
       " 'failed',\n",
       " 'features',\n",
       " 'first',\n",
       " 'five',\n",
       " 'focused',\n",
       " 'found',\n",
       " 'fulfill',\n",
       " 'fully',\n",
       " 'funding',\n",
       " 'generally',\n",
       " 'generic',\n",
       " 'georgetown',\n",
       " 'given',\n",
       " 'gradual',\n",
       " 'grammar',\n",
       " 'hand-written',\n",
       " 'hard',\n",
       " 'head',\n",
       " 'hidden',\n",
       " 'history',\n",
       " 'however',\n",
       " 'human',\n",
       " 'human-like',\n",
       " 'hurts',\n",
       " 'if-then',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'increasingly',\n",
       " 'information',\n",
       " 'input',\n",
       " 'integrated',\n",
       " 'intelligence',\n",
       " 'interaction',\n",
       " 'introduced',\n",
       " 'introduction',\n",
       " 'involved',\n",
       " 'jabberwacky',\n",
       " 'joseph',\n",
       " 'knowledge',\n",
       " 'language',\n",
       " 'larger',\n",
       " 'late',\n",
       " 'law',\n",
       " 'learning',\n",
       " 'lehnert',\n",
       " 'lessening',\n",
       " 'linguistics',\n",
       " 'little',\n",
       " 'machine',\n",
       " 'machine-learning',\n",
       " 'make',\n",
       " 'making',\n",
       " 'many',\n",
       " 'margie',\n",
       " 'markov',\n",
       " 'meehan',\n",
       " 'might',\n",
       " 'models',\n",
       " 'moore',\n",
       " 'much',\n",
       " 'multiple',\n",
       " 'natural',\n",
       " 'notably',\n",
       " 'ontologies',\n",
       " 'pam',\n",
       " 'parry',\n",
       " 'part-of-speech',\n",
       " 'patient',\n",
       " 'periods',\n",
       " 'plot',\n",
       " 'politics',\n",
       " 'power',\n",
       " 'probabilistic',\n",
       " 'problem',\n",
       " 'processing',\n",
       " 'produce',\n",
       " 'produced',\n",
       " 'programmers',\n",
       " 'progress',\n",
       " 'proposed',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'psychotherapist',\n",
       " 'published',\n",
       " 'qualm',\n",
       " 'racter',\n",
       " 'real',\n",
       " 'real-valued',\n",
       " 'real-world',\n",
       " 'recognition',\n",
       " 'reduced',\n",
       " 'reliable',\n",
       " 'rely',\n",
       " 'report',\n",
       " 'research',\n",
       " 'responding',\n",
       " 'response',\n",
       " 'restricted',\n",
       " 'results',\n",
       " 'revolution',\n",
       " 'robust',\n",
       " 'rogerian',\n",
       " 'rules',\n",
       " 'russian',\n",
       " 'sam',\n",
       " 'say',\n",
       " 'schank',\n",
       " 'see',\n",
       " 'sentences',\n",
       " 'sets',\n",
       " 'shrdlu',\n",
       " 'similar',\n",
       " 'simulation',\n",
       " 'sixty',\n",
       " 'slower',\n",
       " 'small',\n",
       " 'soft',\n",
       " 'solved',\n",
       " 'sometimes',\n",
       " 'sort',\n",
       " 'speech',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'startlingly',\n",
       " 'statistical',\n",
       " 'steady',\n",
       " 'structured',\n",
       " 'subtasks',\n",
       " 'successful',\n",
       " 'system',\n",
       " 'systems',\n",
       " 'tagging',\n",
       " 'talespin',\n",
       " 'ten-year-long',\n",
       " 'test',\n",
       " 'theoretical',\n",
       " 'theories',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'time',\n",
       " 'titled',\n",
       " 'transformational',\n",
       " 'translation',\n",
       " 'trees',\n",
       " 'turing',\n",
       " 'underlies',\n",
       " 'underpinnings',\n",
       " 'unfamiliar',\n",
       " 'units',\n",
       " 'upon',\n",
       " 'use',\n",
       " 'using',\n",
       " 'vocabularies',\n",
       " 'weights',\n",
       " 'weizenbaum',\n",
       " 'whose',\n",
       " 'wilensky',\n",
       " 'within',\n",
       " 'work',\n",
       " 'working',\n",
       " 'worlds',\n",
       " 'would',\n",
       " 'write',\n",
       " 'written',\n",
       " 'years'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18802956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whose', 'claimed', 'authors', 'within', 'criterion', 'carbonell', 'units', 'called', 'decision', 'provide', 'weights', 'proposed', 'test', '1950', 'human', 'jabberwacky', 'simulation', 'time', 'data', 'intelligence', '1977', 'hand-written', 'chatterbots', 'theories', 'rely', 'larger', 'systems', 'underlies', 'eliza', 'head', 'worlds', 'produce', '1964', 'comprising', 'earliest-used', 'response', 'three', 'attaching', 'psychotherapist', 'errors', 'theoretical', 'work', 'startlingly', 'results', 'lehnert', 'generic', 'published', 'processing', 'periods', 'complex', '1966.', '``', 'existing', 'algorithms', 'racter', 'underpinnings', 'real-world', '1954', 'making', 'sort', 'almost', 'schank', 'emotion', 'steady', 'input', 'integrated', '1970s', 'learning', '1980s', 'blocks', 'written', '1950s', 'introduced', 'cache', 'gradual', 'alpac', 'restricted', 'base', 'example', 'programmers', 'slower', 'language', 'meehan', 'chomskyan', 'sometimes', 'georgetown', 'e.g', 'computational', 'based', 'history', 'research', 'titled', 'conducted', 'system', 'focused', 'common', 'patient', 'real', 'little', 'corpus', 'thought', 'say', 'models', 'five', 'margie', 'rogerian', 'recognition', 'discouraged', 'tagging', 'much', 'due', 'article', 'fulfill', 'would', '1979', 'starting', 'russian', 'sam', 'linguistics', 'grammar', 'hidden', 'real-valued', 'transformational', 'might', 'contains', 'plot', 'make', 'fully', 'working', 'turing', 'moore', 'human-like', '3', 'robust', 'failed', 'features', 'alan', 'see', 'small', 'sixty', 'information', 'many', 'wilensky', 'reliable', 'machine-learning', 'pam', 'experiment', 'successful', 'expectations', 'conceptual', 'however', 'similar', 'probabilistic', 'politics', 'law', '1975', 'hurts', 'ontologies', 'power', 'markov', 'upon', 'ten-year-long', '1981', '2', '1960s', 'structured', 'years', 'although', 'subtasks', 'english', 'rules', 'started', 'trees', \"''\", 'use', 'decisions', 'multiple', 'using', 'reduced', 'solved', 'progress', 'weizenbaum', 'if-then', 'first', 'sentences', 'notably', 'exceeded', 'funding', 'soft', 'responding', '1978', 'write', 'examples', 'translation', 'late', 'approach', 'including', 'dramatically', 'report', 'unfamiliar', 'problem', 'knowledge', 'hard', 'computer-understandable', 'natural', 'statistical', 'cullingford', 'revolution', 'provided', 'introduction', 'given', 'began', 'earlier', 'talespin', 'qualm', 'dominance', 'especially', 'automatic', 'developed', 'parry', 'speech', 'involved', 'joseph', 'sets', 'machine', 'increase', 'found', 'shrdlu', '1976', 'vocabularies', 'interaction', 'produced', 'lessening', '1966', \"'s\", 'increasingly', 'generally', 'part-of-speech'}\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation\n",
    "set_wo_punct = set_wo_stopwords - set(punctuation)\n",
    "print(set_wo_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950109a",
   "metadata": {},
   "source": [
    "### Step 6: Normalize the text using stemming and/or lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a47c46e",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53cfb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whose', 'claim', 'author', 'within', 'criterion', 'carbonel', 'unit', 'call', 'decis', 'provid', 'weight', 'propos', 'test', '1950', 'human', 'jabberwacki', 'simul', 'time', 'data', 'intellig', '1977', 'hand-written', 'chatterbot', 'theori', 'reli', 'larger', 'system', 'under', 'eliza', 'head', 'world', 'produc', '1964', 'compris', 'earliest-us', 'respons', 'three', 'attach', 'psychotherapist', 'error', 'theoret', 'work', 'startl', 'result', 'lehnert', 'generic', 'publish', 'process', 'period', 'complex', '1966.', '``', 'exist', 'algorithm', 'racter', 'underpin', 'real-world', '1954', 'make', 'sort', 'almost', 'schank', 'emot', 'steadi', 'input', 'integr', '1970s', 'learn', '1980s', 'block', 'written', '1950s', 'introduc', 'cach', 'gradual', 'alpac', 'restrict', 'base', 'exampl', 'programm', 'slower', 'languag', 'meehan', 'chomskyan', 'sometim', 'georgetown', 'e.g', 'comput', 'base', 'histori', 'research', 'titl', 'conduct', 'system', 'focus', 'common', 'patient', 'real', 'littl', 'corpus', 'thought', 'say', 'model', 'five', 'margi', 'rogerian', 'recognit', 'discourag', 'tag', 'much', 'due', 'articl', 'fulfil', 'would', '1979', 'start', 'russian', 'sam', 'linguist', 'grammar', 'hidden', 'real-valu', 'transform', 'might', 'contain', 'plot', 'make', 'fulli', 'work', 'ture', 'moor', 'human-lik', '3', 'robust', 'fail', 'featur', 'alan', 'see', 'small', 'sixti', 'inform', 'mani', 'wilenski', 'reliabl', 'machine-learn', 'pam', 'experi', 'success', 'expect', 'conceptu', 'howev', 'similar', 'probabilist', 'polit', 'law', '1975', 'hurt', 'ontolog', 'power', 'markov', 'upon', 'ten-year-long', '1981', '2', '1960s', 'structur', 'year', 'although', 'subtask', 'english', 'rule', 'start', 'tree', \"''\", 'use', 'decis', 'multipl', 'use', 'reduc', 'solv', 'progress', 'weizenbaum', 'if-then', 'first', 'sentenc', 'notabl', 'exceed', 'fund', 'soft', 'respond', '1978', 'write', 'exampl', 'translat', 'late', 'approach', 'includ', 'dramat', 'report', 'unfamiliar', 'problem', 'knowledg', 'hard', 'computer-understand', 'natur', 'statist', 'cullingford', 'revolut', 'provid', 'introduct', 'given', 'began', 'earlier', 'talespin', 'qualm', 'domin', 'especi', 'automat', 'develop', 'parri', 'speech', 'involv', 'joseph', 'set', 'machin', 'increas', 'found', 'shrdlu', '1976', 'vocabulari', 'interact', 'produc', 'lessen', '1966', \"'s\", 'increas', 'general', 'part-of-speech']\n"
     ]
    }
   ],
   "source": [
    "stemmed_list = []\n",
    "stemObj = SnowballStemmer(\"english\")\n",
    "for i in set_wo_punct:\n",
    "    stemmed_list.append(stemObj.stem(i))\n",
    "    \n",
    "print(stemmed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c2b78",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ead7f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whose', 'WP$'), ('claimed', 'JJ'), ('authors', 'NNS'), ('within', 'IN'), ('criterion', 'NN'), ('carbonell', 'VBP'), ('units', 'NNS'), ('called', 'VBN'), ('decision', 'NN'), ('provide', 'NN'), ('weights', 'NNS'), ('proposed', 'VBN'), ('test', 'NN'), ('1950', 'CD'), ('human', 'JJ'), ('jabberwacky', 'JJ'), ('simulation', 'NN'), ('time', 'NN'), ('data', 'NNS'), ('intelligence', 'NN'), ('1977', 'CD'), ('hand-written', 'JJ'), ('chatterbots', 'NNS'), ('theories', 'NNS'), ('rely', 'RB'), ('larger', 'JJR'), ('systems', 'NNS'), ('underlies', 'NNS'), ('eliza', 'VBP'), ('head', 'NN'), ('worlds', 'NNS'), ('produce', 'VBP'), ('1964', 'CD'), ('comprising', 'VBG'), ('earliest-used', 'JJ'), ('response', 'NN'), ('three', 'CD'), ('attaching', 'VBG'), ('psychotherapist', 'JJ'), ('errors', 'NNS'), ('theoretical', 'JJ'), ('work', 'NN'), ('startlingly', 'RB'), ('results', 'NNS'), ('lehnert', 'JJ'), ('generic', 'JJ'), ('published', 'VBN'), ('processing', 'NN'), ('periods', 'NNS'), ('complex', 'JJ'), ('1966.', 'CD'), ('``', '``'), ('existing', 'VBG'), ('algorithms', 'RB'), ('racter', 'JJ'), ('underpinnings', 'NNS'), ('real-world', 'JJ'), ('1954', 'CD'), ('making', 'VBG'), ('sort', 'NN'), ('almost', 'RB'), ('schank', 'JJ'), ('emotion', 'NN'), ('steady', 'JJ'), ('input', 'NN'), ('integrated', 'VBD'), ('1970s', 'CD'), ('learning', 'VBG'), ('1980s', 'CD'), ('blocks', 'NNS'), ('written', 'VBN'), ('1950s', 'CD'), ('introduced', 'JJ'), ('cache', 'NN'), ('gradual', 'JJ'), ('alpac', 'NN'), ('restricted', 'VBD'), ('base', 'JJ'), ('example', 'NN'), ('programmers', 'NNS'), ('slower', 'JJR'), ('language', 'NN'), ('meehan', 'NN'), ('chomskyan', 'NN'), ('sometimes', 'RB'), ('georgetown', 'JJ'), ('e.g', 'JJ'), ('computational', 'NN'), ('based', 'VBN'), ('history', 'NN'), ('research', 'NN'), ('titled', 'VBN'), ('conducted', 'JJ'), ('system', 'NN'), ('focused', 'VBD'), ('common', 'JJ'), ('patient', 'JJ'), ('real', 'JJ'), ('little', 'JJ'), ('corpus', 'NN'), ('thought', 'VBD'), ('say', 'VBP'), ('models', 'NNS'), ('five', 'CD'), ('margie', 'NNS'), ('rogerian', 'JJ'), ('recognition', 'NN'), ('discouraged', 'VBD'), ('tagging', 'VBG'), ('much', 'JJ'), ('due', 'JJ'), ('article', 'NN'), ('fulfill', 'NN'), ('would', 'MD'), ('1979', 'CD'), ('starting', 'VBG'), ('russian', 'JJ'), ('sam', 'NN'), ('linguistics', 'NNS'), ('grammar', 'VBP'), ('hidden', 'JJ'), ('real-valued', 'JJ'), ('transformational', 'NN'), ('might', 'MD'), ('contains', 'VB'), ('plot', 'JJ'), ('make', 'VB'), ('fully', 'RB'), ('working', 'VBG'), ('turing', 'VBG'), ('moore', 'RB'), ('human-like', 'JJ'), ('3', 'CD'), ('robust', 'NN'), ('failed', 'VBD'), ('features', 'NNS'), ('alan', 'VBP'), ('see', 'VB'), ('small', 'JJ'), ('sixty', 'JJ'), ('information', 'NN'), ('many', 'JJ'), ('wilensky', 'NN'), ('reliable', 'JJ'), ('machine-learning', 'JJ'), ('pam', 'NN'), ('experiment', 'NN'), ('successful', 'JJ'), ('expectations', 'NNS'), ('conceptual', 'VBP'), ('however', 'RB'), ('similar', 'JJ'), ('probabilistic', 'JJ'), ('politics', 'NNS'), ('law', 'NN'), ('1975', 'CD'), ('hurts', 'VBZ'), ('ontologies', 'NNS'), ('power', 'NN'), ('markov', 'VBP'), ('upon', 'IN'), ('ten-year-long', 'JJ'), ('1981', 'CD'), ('2', 'CD'), ('1960s', 'CD'), ('structured', 'JJ'), ('years', 'NNS'), ('although', 'IN'), ('subtasks', 'JJ'), ('english', 'JJ'), ('rules', 'NNS'), ('started', 'VBD'), ('trees', 'NNS'), (\"''\", \"''\"), ('use', 'NN'), ('decisions', 'NNS'), ('multiple', 'VBP'), ('using', 'VBG'), ('reduced', 'VBN'), ('solved', 'JJ'), ('progress', 'NN'), ('weizenbaum', 'VBD'), ('if-then', 'JJ'), ('first', 'JJ'), ('sentences', 'NNS'), ('notably', 'RB'), ('exceeded', 'VBD'), ('funding', 'NN'), ('soft', 'JJ'), ('responding', 'VBG'), ('1978', 'CD'), ('write', 'JJ'), ('examples', 'NNS'), ('translation', 'NN'), ('late', 'RB'), ('approach', 'NN'), ('including', 'VBG'), ('dramatically', 'RB'), ('report', 'NN'), ('unfamiliar', 'JJ'), ('problem', 'NN'), ('knowledge', 'NN'), ('hard', 'RB'), ('computer-understandable', 'JJ'), ('natural', 'JJ'), ('statistical', 'JJ'), ('cullingford', 'NN'), ('revolution', 'NN'), ('provided', 'VBD'), ('introduction', 'NN'), ('given', 'VBN'), ('began', 'VBD'), ('earlier', 'RBR'), ('talespin', 'JJ'), ('qualm', 'JJ'), ('dominance', 'NN'), ('especially', 'RB'), ('automatic', 'JJ'), ('developed', 'VBD'), ('parry', 'JJ'), ('speech', 'NN'), ('involved', 'VBN'), ('joseph', 'JJ'), ('sets', 'NNS'), ('machine', 'NN'), ('increase', 'NN'), ('found', 'VBD'), ('shrdlu', 'JJ'), ('1976', 'CD'), ('vocabularies', 'NNS'), ('interaction', 'NN'), ('produced', 'VBD'), ('lessening', 'JJ'), ('1966', 'CD'), (\"'s\", 'POS'), ('increasingly', 'RB'), ('generally', 'RB'), ('part-of-speech', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "#parts of speech\n",
    "pos_tag_list = pos_tag(set_wo_punct)\n",
    "print(pos_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe483044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whose', 'claimed', 'author', 'within', 'criterion', 'carbonell', 'unit', 'call', 'decision', 'provide', 'weight', 'propose', 'test', '1950', 'human', 'jabberwacky', 'simulation', 'time', 'data', 'intelligence', '1977', 'hand-written', 'chatterbots', 'theory', 'rely', 'large', 'system', 'underlies', 'eliza', 'head', 'world', 'produce', '1964', 'comprise', 'earliest-used', 'response', 'three', 'attach', 'psychotherapist', 'error', 'theoretical', 'work', 'startlingly', 'result', 'lehnert', 'generic', 'publish', 'processing', 'period', 'complex', '1966.', '``', 'exist', 'algorithms', 'racter', 'underpinnings', 'real-world', '1954', 'make', 'sort', 'almost', 'schank', 'emotion', 'steady', 'input', 'integrate', '1970s', 'learn', '1980s', 'block', 'write', '1950s', 'introduced', 'cache', 'gradual', 'alpac', 'restrict', 'base', 'example', 'programmer', 'slow', 'language', 'meehan', 'chomskyan', 'sometimes', 'georgetown', 'e.g', 'computational', 'base', 'history', 'research', 'title', 'conducted', 'system', 'focus', 'common', 'patient', 'real', 'little', 'corpus', 'think', 'say', 'model', 'five', 'margie', 'rogerian', 'recognition', 'discourage', 'tag', 'much', 'due', 'article', 'fulfill', 'would', '1979', 'start', 'russian', 'sam', 'linguistics', 'grammar', 'hidden', 'real-valued', 'transformational', 'might', 'contain', 'plot', 'make', 'fully', 'work', 'turing', 'moore', 'human-like', '3', 'robust', 'fail', 'feature', 'alan', 'see', 'small', 'sixty', 'information', 'many', 'wilensky', 'reliable', 'machine-learning', 'pam', 'experiment', 'successful', 'expectation', 'conceptual', 'however', 'similar', 'probabilistic', 'politics', 'law', '1975', 'hurt', 'ontology', 'power', 'markov', 'upon', 'ten-year-long', '1981', '2', '1960s', 'structured', 'year', 'although', 'subtasks', 'english', 'rule', 'start', 'tree', \"''\", 'use', 'decision', 'multiple', 'use', 'reduce', 'solved', 'progress', 'weizenbaum', 'if-then', 'first', 'sentence', 'notably', 'exceed', 'funding', 'soft', 'respond', '1978', 'write', 'example', 'translation', 'late', 'approach', 'include', 'dramatically', 'report', 'unfamiliar', 'problem', 'knowledge', 'hard', 'computer-understandable', 'natural', 'statistical', 'cullingford', 'revolution', 'provide', 'introduction', 'give', 'begin', 'earlier', 'talespin', 'qualm', 'dominance', 'especially', 'automatic', 'develop', 'parry', 'speech', 'involve', 'joseph', 'set', 'machine', 'increase', 'find', 'shrdlu', '1976', 'vocabulary', 'interaction', 'produce', 'lessening', '1966', \"'s\", 'increasingly', 'generally', 'part-of-speech']\n"
     ]
    }
   ],
   "source": [
    "#for getting the parts of speech\n",
    "from nltk import wordnet\n",
    "\n",
    "#Lemmatization\n",
    "lemma_list = []\n",
    "lemmaObj = WordNetLemmatizer()\n",
    "for word, pos in pos_tag_list:\n",
    "    if pos.startswith('J'):  # Adjective\n",
    "        lemma = lemmaObj.lemmatize(word, 'a')\n",
    "    elif pos.startswith('V'):  # Verb\n",
    "        lemma = lemmaObj.lemmatize(word, 'v')\n",
    "    elif pos.startswith('N'):  # Noun\n",
    "        lemma = lemmaObj.lemmatize(word, 'n')\n",
    "    elif pos.startswith('R'):  # Adverb\n",
    "        lemma = lemmaObj.lemmatize(word, 'r')\n",
    "    else:\n",
    "        lemma = word\n",
    "    lemma_list.append(lemma)\n",
    "        \n",
    "print(lemma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162d022",
   "metadata": {},
   "source": [
    "### Step 7: Create n-grams from text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdf22cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whose', 'claimed'), ('claimed', 'authors'), ('authors', 'within'), ('within', 'criterion'), ('criterion', 'carbonell'), ('carbonell', 'units'), ('units', 'called'), ('called', 'decision'), ('decision', 'provide'), ('provide', 'weights'), ('weights', 'proposed'), ('proposed', 'test'), ('test', '1950'), ('1950', 'human'), ('human', 'jabberwacky'), ('jabberwacky', 'simulation'), ('simulation', 'time'), ('time', 'data'), ('data', 'intelligence'), ('intelligence', '1977'), ('1977', 'hand-written'), ('hand-written', 'chatterbots'), ('chatterbots', 'theories'), ('theories', 'rely'), ('rely', 'larger'), ('larger', 'systems'), ('systems', 'underlies'), ('underlies', 'eliza'), ('eliza', 'head'), ('head', 'worlds'), ('worlds', 'produce'), ('produce', '1964'), ('1964', 'comprising'), ('comprising', 'earliest-used'), ('earliest-used', 'response'), ('response', 'three'), ('three', 'attaching'), ('attaching', 'psychotherapist'), ('psychotherapist', 'errors'), ('errors', 'theoretical'), ('theoretical', 'work'), ('work', 'startlingly'), ('startlingly', 'results'), ('results', 'lehnert'), ('lehnert', 'generic'), ('generic', 'published'), ('published', 'processing'), ('processing', 'periods'), ('periods', 'complex'), ('complex', '1966.'), ('1966.', '``'), ('``', 'existing'), ('existing', 'algorithms'), ('algorithms', 'racter'), ('racter', 'underpinnings'), ('underpinnings', 'real-world'), ('real-world', '1954'), ('1954', 'making'), ('making', 'sort'), ('sort', 'almost'), ('almost', 'schank'), ('schank', 'emotion'), ('emotion', 'steady'), ('steady', 'input'), ('input', 'integrated'), ('integrated', '1970s'), ('1970s', 'learning'), ('learning', '1980s'), ('1980s', 'blocks'), ('blocks', 'written'), ('written', '1950s'), ('1950s', 'introduced'), ('introduced', 'cache'), ('cache', 'gradual'), ('gradual', 'alpac'), ('alpac', 'restricted'), ('restricted', 'base'), ('base', 'example'), ('example', 'programmers'), ('programmers', 'slower'), ('slower', 'language'), ('language', 'meehan'), ('meehan', 'chomskyan'), ('chomskyan', 'sometimes'), ('sometimes', 'georgetown'), ('georgetown', 'e.g'), ('e.g', 'computational'), ('computational', 'based'), ('based', 'history'), ('history', 'research'), ('research', 'titled'), ('titled', 'conducted'), ('conducted', 'system'), ('system', 'focused'), ('focused', 'common'), ('common', 'patient'), ('patient', 'real'), ('real', 'little'), ('little', 'corpus'), ('corpus', 'thought'), ('thought', 'say'), ('say', 'models'), ('models', 'five'), ('five', 'margie'), ('margie', 'rogerian'), ('rogerian', 'recognition'), ('recognition', 'discouraged'), ('discouraged', 'tagging'), ('tagging', 'much'), ('much', 'due'), ('due', 'article'), ('article', 'fulfill'), ('fulfill', 'would'), ('would', '1979'), ('1979', 'starting'), ('starting', 'russian'), ('russian', 'sam'), ('sam', 'linguistics'), ('linguistics', 'grammar'), ('grammar', 'hidden'), ('hidden', 'real-valued'), ('real-valued', 'transformational'), ('transformational', 'might'), ('might', 'contains'), ('contains', 'plot'), ('plot', 'make'), ('make', 'fully'), ('fully', 'working'), ('working', 'turing'), ('turing', 'moore'), ('moore', 'human-like'), ('human-like', '3'), ('3', 'robust'), ('robust', 'failed'), ('failed', 'features'), ('features', 'alan'), ('alan', 'see'), ('see', 'small'), ('small', 'sixty'), ('sixty', 'information'), ('information', 'many'), ('many', 'wilensky'), ('wilensky', 'reliable'), ('reliable', 'machine-learning'), ('machine-learning', 'pam'), ('pam', 'experiment'), ('experiment', 'successful'), ('successful', 'expectations'), ('expectations', 'conceptual'), ('conceptual', 'however'), ('however', 'similar'), ('similar', 'probabilistic'), ('probabilistic', 'politics'), ('politics', 'law'), ('law', '1975'), ('1975', 'hurts'), ('hurts', 'ontologies'), ('ontologies', 'power'), ('power', 'markov'), ('markov', 'upon'), ('upon', 'ten-year-long'), ('ten-year-long', '1981'), ('1981', '2'), ('2', '1960s'), ('1960s', 'structured'), ('structured', 'years'), ('years', 'although'), ('although', 'subtasks'), ('subtasks', 'english'), ('english', 'rules'), ('rules', 'started'), ('started', 'trees'), ('trees', \"''\"), (\"''\", 'use'), ('use', 'decisions'), ('decisions', 'multiple'), ('multiple', 'using'), ('using', 'reduced'), ('reduced', 'solved'), ('solved', 'progress'), ('progress', 'weizenbaum'), ('weizenbaum', 'if-then'), ('if-then', 'first'), ('first', 'sentences'), ('sentences', 'notably'), ('notably', 'exceeded'), ('exceeded', 'funding'), ('funding', 'soft'), ('soft', 'responding'), ('responding', '1978'), ('1978', 'write'), ('write', 'examples'), ('examples', 'translation'), ('translation', 'late'), ('late', 'approach'), ('approach', 'including'), ('including', 'dramatically'), ('dramatically', 'report'), ('report', 'unfamiliar'), ('unfamiliar', 'problem'), ('problem', 'knowledge'), ('knowledge', 'hard'), ('hard', 'computer-understandable'), ('computer-understandable', 'natural'), ('natural', 'statistical'), ('statistical', 'cullingford'), ('cullingford', 'revolution'), ('revolution', 'provided'), ('provided', 'introduction'), ('introduction', 'given'), ('given', 'began'), ('began', 'earlier'), ('earlier', 'talespin'), ('talespin', 'qualm'), ('qualm', 'dominance'), ('dominance', 'especially'), ('especially', 'automatic'), ('automatic', 'developed'), ('developed', 'parry'), ('parry', 'speech'), ('speech', 'involved'), ('involved', 'joseph'), ('joseph', 'sets'), ('sets', 'machine'), ('machine', 'increase'), ('increase', 'found'), ('found', 'shrdlu'), ('shrdlu', '1976'), ('1976', 'vocabularies'), ('vocabularies', 'interaction'), ('interaction', 'produced'), ('produced', 'lessening'), ('lessening', '1966'), ('1966', \"'s\"), (\"'s\", 'increasingly'), ('increasingly', 'generally'), ('generally', 'part-of-speech')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "bigrams = ngrams(set_wo_punct, 2)\n",
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e517a12",
   "metadata": {},
   "source": [
    "## Regular Expressions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc7050c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e6c7a",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199323e3",
   "metadata": {},
   "source": [
    "Search method searches for the string present in the r\"\" in the whole input string and returns a match object if there is a match else returns None. Search only returns the first match present in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e29768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Occurances of a-z:  <re.Match object; span=(5, 8), match='was'>\n"
     ]
    }
   ],
   "source": [
    "sent= \"1947 was when India got their independence. \"\n",
    "print(\" Occurances of a-z: \", re.search(r\"[a-z]+\", sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a10cd",
   "metadata": {},
   "source": [
    "The match at the string \"1947\" is the only string according to search method because our set only ranges from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9b0cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Occurances of 0-9:  <re.Match object; span=(0, 4), match='1947'>\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"1947 was when India became independent.\"\n",
    "print(\" Occurances of 0-9: \", re.search(r\"[0-9]+\", sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af344399",
   "metadata": {},
   "source": [
    "Our desired set is namely a-z, A-Z, 0-9, '_' and a space character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1868c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurances of w and space:  <re.Match object; span=(0, 38), match='1947_was when India became independent'>\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"1947_was when India became independent.\"\n",
    "print(\"Occurances of w and space: \", re.search(r\"[\\w ]+\", sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b5839",
   "metadata": {},
   "source": [
    "# substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910302c",
   "metadata": {},
   "source": [
    "Sub is substitution of a substring with another string in the given input string. So understandably it takes three parameters. \n",
    "\n",
    "First argument is the string to be removed, second argument is the resultant string and the last argument is the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb6f3a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like tea\n"
     ]
    }
   ],
   "source": [
    "sent = \"I like coffee\" \n",
    "print(re.sub(r\"coffee\",\"tea\",sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9867a",
   "metadata": {},
   "source": [
    "## Findall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2753fc3",
   "metadata": {},
   "source": [
    "Findall parses our input string from left to right and returns all the substrings matching with our raw string as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20e18578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coffee']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(re.findall(r\"coffee\",sent))\n",
    "\n",
    "print(len(re.findall(r\"coffee\",sent)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10583ee3",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1755a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Hussein/NNP Obama/NNP)\n",
      "  (/(\n",
      "  born/VBN\n",
      "  August/NNP\n",
      "  4/CD\n",
      "  ,/,\n",
      "  1961/CD\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  politician/NN\n",
      "  who/WP\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  the/DT\n",
      "  44th/CD\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  from/IN\n",
      "  January/NNP\n",
      "  20/CD\n",
      "  ,/,\n",
      "  2009/CD\n",
      "  ,/,\n",
      "  to/TO\n",
      "  January/NNP\n",
      "  20/CD\n",
      "  ,/,\n",
      "  2017/CD\n",
      "  ./.\n",
      "  A/DT\n",
      "  member/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Democratic/NNP Party/NNP)\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  (ORGANIZATION African/JJ American/NNP)\n",
      "  to/TO\n",
      "  assume/VB\n",
      "  the/DT\n",
      "  presidency/NN\n",
      "  and/CC\n",
      "  previously/RB\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  a/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  Senator/NNP\n",
      "  from/IN\n",
      "  (GPE Illinois/NNP)\n",
      "  (/(\n",
      "  20052008/CD\n",
      "  )/)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\GOD\n",
      "[nltk_data]     WORLD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\GOD\n",
      "[nltk_data]     WORLD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "barack = \"\"\"Barack Hussein Obama (born August 4, 1961) is an American politician \n",
    "who served as the 44th President of the United States from January 20, 2009, to January 20, 2017.\n",
    "A member of the Democratic Party, he was the first African American to assume the presidency \n",
    "and previously served as a United States Senator from Illinois (20052008).\"\"\"\n",
    "\n",
    "token_barack = word_tokenize(barack)\n",
    "pos_list = pos_tag(token_barack)\n",
    "print(ne_chunk(pos_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a36a65",
   "metadata": {},
   "source": [
    "The below code demonstrates the usage of RegexpParser which can give a more desirable result in comparison to the default NE Chunker. Here we need to configure how entities are determined, i.e. what kind of POS combinations results in a specific named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b1f7986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Person Barack/NNP Hussein/NNP Obama/NNP)\n",
      "  (/(\n",
      "  born/VBN\n",
      "  (Date August/NNP 4/CD ,/, 1961/CD)\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  American/JJ\n",
      "  politician/NN\n",
      "  who/WP\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  the/DT\n",
      "  44th/CD\n",
      "  (Person President/NNP)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Place United/NNP States/NNPS)\n",
      "  from/IN\n",
      "  (Date January/NNP 20/CD ,/, 2009/CD)\n",
      "  ,/,\n",
      "  to/TO\n",
      "  (Date January/NNP 20/CD ,/, 2017/CD)\n",
      "  ./.\n",
      "  A/DT\n",
      "  member/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (Person Democratic/NNP Party/NNP)\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  African/JJ\n",
      "  (Person American/NNP)\n",
      "  to/TO\n",
      "  assume/VB\n",
      "  the/DT\n",
      "  presidency/NN\n",
      "  and/CC\n",
      "  previously/RB\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  a/DT\n",
      "  (Place United/NNP States/NNPS)\n",
      "  (Person Senator/NNP)\n",
      "  from/IN\n",
      "  (Person Illinois/NNP)\n",
      "  (/(\n",
      "  20052008/CD\n",
      "  )/)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpParser\n",
    "grammar = r\"\"\"Place: {<NNP><NNPS>+}\n",
    "           Date: {<NNP><CD><,><CD>}\n",
    "           Person: {<NNP>+}\"\"\"\n",
    "regParser = RegexpParser(grammar)\n",
    "reg_lines = regParser.parse(pos_list)\n",
    "print(reg_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08254faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = \"\"\"Donald John Trump (born June 14, 1946) is the 45th and current President of the United States.\n",
    "Before entering politics, he was a businessman and television personality. \n",
    "Trump was born and raised in the New York City borough of Queens, and received an economics degree from the\n",
    " Wharton School of the University of Pennsylvania. \n",
    "He took charge of his family's real estate business in 1971, renamed it The Trump Organization, and expanded \n",
    "it from Queens and Brooklyn into Manhattan. \n",
    "The company built or renovated skyscrapers, hotels, casinos, and golf courses. \n",
    "Trump later started various side ventures, including licensing his name for real estate and consumer products.\n",
    "He managed the company until his 2017 inauguration. \n",
    "He co-authored several books, including The Art of the Deal. He owned the Miss Universe and Miss USA beauty \n",
    "pageants from 1996 to 2015, and he produced and hosted the reality television show The Apprentice from 2003 to 2015.\n",
    "Forbes estimates his net worth to be $3.1 billion.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44045953",
   "metadata": {},
   "source": [
    "# Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "012b0847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('race', 'NN'), ('offcials', 'NNS'), ('refused', 'VBD'), ('to', 'TO'), ('permit', 'VB'), ('the', 'DT'), ('team', 'NN'), ('to', 'TO'), ('race', 'NN'), ('roday', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent1 = \" The race offcials refused to permit the team to race roday\"\n",
    "print(pos_tag(word_tokenize(sent1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4bc7a9",
   "metadata": {},
   "source": [
    "Here, both the races are classified as NOUN which is incorrect. Hence, it is making the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfade536",
   "metadata": {},
   "source": [
    "## Default Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46fb84f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\GOD\n",
      "[nltk_data]     WORLD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "#brown is a corpus\n",
    "\n",
    "# Trying to get the most common tag in the brown corpus\n",
    "\n",
    "tags = [tag for (word,tag) in brown.tagged_words()]\n",
    "most_common_tag = nltk.FreqDist(tags).max()\n",
    "print(most_common_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c0a04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack', 'NN'), ('Hussein', 'NN'), ('Obama', 'NN'), ('(', 'NN'), ('born', 'NN'), ('August', 'NN'), ('4', 'NN'), (',', 'NN'), ('1961', 'NN'), (')', 'NN'), ('is', 'NN'), ('an', 'NN'), ('American', 'NN'), ('politician', 'NN'), ('who', 'NN'), ('served', 'NN'), ('as', 'NN'), ('the', 'NN'), ('44th', 'NN'), ('President', 'NN'), ('of', 'NN'), ('the', 'NN'), ('United', 'NN'), ('States', 'NN'), ('from', 'NN'), ('January', 'NN'), ('20', 'NN'), (',', 'NN'), ('2009', 'NN'), (',', 'NN'), ('to', 'NN'), ('January', 'NN'), ('20', 'NN'), (',', 'NN'), ('2017', 'NN'), ('.', 'NN'), ('A', 'NN'), ('member', 'NN'), ('of', 'NN'), ('the', 'NN'), ('Democratic', 'NN'), ('Party', 'NN'), (',', 'NN'), ('he', 'NN'), ('was', 'NN'), ('the', 'NN'), ('first', 'NN'), ('African', 'NN'), ('American', 'NN'), ('to', 'NN'), ('assume', 'NN'), ('the', 'NN'), ('presidency', 'NN'), ('and', 'NN'), ('previously', 'NN'), ('served', 'NN'), ('as', 'NN'), ('a', 'NN'), ('United', 'NN'), ('States', 'NN'), ('Senator', 'NN'), ('from', 'NN'), ('Illinois', 'NN'), ('(', 'NN'), ('20052008', 'NN'), (')', 'NN'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Use the most common tagger as the input for the default tagger\n",
    "from nltk import DefaultTagger\n",
    "default_tag = DefaultTagger(most_common_tag)\n",
    "def_tagged_barack = default_tag.tag(token_barack)\n",
    "print(def_tagged_barack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754697d1",
   "metadata": {},
   "source": [
    "## Lookup taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6800691",
   "metadata": {},
   "source": [
    "A NgramTagger tags a word based on the previous n words occurring in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bdc0b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"the quick brown fox jumps over the lazy dog\"\n",
    "training = pos_tag(word_tokenize(sent1))\n",
    "print(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9742714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us train the n gram tagger\n",
    "ngram_tagger = nltk.NgramTagger(n=2, train = [training])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb90595b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('was', None), ('jumped', None), ('over', None), ('by', None), ('the', None), ('quick', None), ('brown', None), ('fox', None)]\n"
     ]
    }
   ],
   "source": [
    "sent2 = \"the lazy dog was jumped over by the quick brown fox\"\n",
    "tag = ngram_tagger.tag(word_tokenize(sent2))\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c867c",
   "metadata": {},
   "source": [
    "This looking up of occurrence of words in the sequence appearing in the training set can be considered as the context.\n",
    "\n",
    "Therefore, we can now understand that a NgramTagger tags words that appear in context, and the context is defined by the window 'n' which is the number of tokens to consider together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5279821",
   "metadata": {},
   "source": [
    "## Example of tagging pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323fb08",
   "metadata": {},
   "source": [
    "### <b> UnigramTagger --> RegexpTagger --> DefaultTagger. Note that --> indicates backoff.<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd44cb72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Donald', 'NN'), ('John', 'NN'), ('Trump', 'NN'), ('(', '('), ('born', 'VBN'), ('June', 'NN'), ('14', 'CD'), (',', ','), ('1946', 'CD'), (')', ')'), ('is', 'VBZ'), ('the', 'DT'), ('45th', 'NN'), ('and', 'CC'), ('current', 'NN'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.'), ('Before', 'NN'), ('entering', 'VBG'), ('politics', 'NNS'), (',', ','), ('he', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('businessman', 'NN'), ('and', 'CC'), ('television', 'NN'), ('personality', 'NN'), ('.', '.'), ('Trump', 'NN'), ('was', 'VBD'), ('born', 'VBN'), ('and', 'CC'), ('raised', 'VBD'), ('in', 'NN'), ('the', 'DT'), ('New', 'NN'), ('York', 'NN'), ('City', 'NN'), ('borough', 'NN'), ('of', 'IN'), ('Queens', 'NNS'), (',', ','), ('and', 'CC'), ('received', 'VBD'), ('an', 'DT'), ('economics', 'NNS'), ('degree', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Wharton', 'NN'), ('School', 'NN'), ('of', 'IN'), ('the', 'DT'), ('University', 'NN'), ('of', 'IN'), ('Pennsylvania', 'NN'), ('.', '.'), ('He', 'NN'), ('took', 'NN'), ('charge', 'NN'), ('of', 'IN'), ('his', 'NNS'), ('family', 'NN'), (\"'s\", 'NN$'), ('real', 'NN'), ('estate', 'NN'), ('business', 'NNS'), ('in', 'NN'), ('1971', 'CD'), (',', ','), ('renamed', 'VBD'), ('it', 'NN'), ('The', 'NN'), ('Trump', 'NN'), ('Organization', 'NN'), (',', ','), ('and', 'CC'), ('expanded', 'VBD'), ('it', 'NN'), ('from', 'IN'), ('Queens', 'NNS'), ('and', 'CC'), ('Brooklyn', 'NN'), ('into', 'NN'), ('Manhattan', 'NN'), ('.', '.'), ('The', 'NN'), ('company', 'NN'), ('built', 'NN'), ('or', 'NN'), ('renovated', 'VBD'), ('skyscrapers', 'NNS'), (',', ','), ('hotels', 'NNS'), (',', ','), ('casinos', 'NNS'), (',', ','), ('and', 'CC'), ('golf', 'NN'), ('courses', 'VBZ'), ('.', '.'), ('Trump', 'NN'), ('later', 'NN'), ('started', 'VBD'), ('various', 'NNS'), ('side', 'NN'), ('ventures', 'VBZ'), (',', ','), ('including', 'VBG'), ('licensing', 'VBG'), ('his', 'NNS'), ('name', 'NN'), ('for', 'NN'), ('real', 'NN'), ('estate', 'NN'), ('and', 'CC'), ('consumer', 'NN'), ('products', 'NNS'), ('.', '.'), ('He', 'NN'), ('managed', 'VBD'), ('the', 'DT'), ('company', 'NN'), ('until', 'NN'), ('his', 'NNS'), ('2017', 'CD'), ('inauguration', 'NN'), ('.', '.'), ('He', 'NN'), ('co-authored', 'VBD'), ('several', 'NN'), ('books', 'NNS'), (',', ','), ('including', 'VBG'), ('The', 'NN'), ('Art', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Deal', 'NN'), ('.', '.'), ('He', 'NN'), ('owned', 'VBD'), ('the', 'DT'), ('Miss', 'NNS'), ('Universe', 'NN'), ('and', 'CC'), ('Miss', 'NNS'), ('USA', 'NN'), ('beauty', 'NN'), ('pageants', 'NNS'), ('from', 'IN'), ('1996', 'CD'), ('to', 'TO'), ('2015', 'CD'), (',', ','), ('and', 'CC'), ('he', 'PRP'), ('produced', 'VBD'), ('and', 'CC'), ('hosted', 'VBD'), ('the', 'DT'), ('reality', 'NN'), ('television', 'NN'), ('show', 'NN'), ('The', 'NN'), ('Apprentice', 'NN'), ('from', 'IN'), ('2003', 'CD'), ('to', 'TO'), ('2015', 'CD'), ('.', '.'), ('Forbes', 'VBZ'), ('estimates', 'VBZ'), ('his', 'NNS'), ('net', 'NN'), ('worth', 'NN'), ('to', 'TO'), ('be', 'NN'), ('$', 'NN'), ('3.1', 'CD'), ('billion', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "default_tag = DefaultTagger('NN')\n",
    "patterns = [\n",
    "    (r'.*\\'s$', 'NN$'), #possessive nouns\n",
    "    (r'.*es$','VBZ'),\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
    "    (r'[Aa][Nn][Dd]','CC'),\n",
    "    (r'.*ed$', 'VBD'),\n",
    "    (r',' , ','),\n",
    "    (r'.*ould$', 'MD'),\n",
    "    (r'.*ing$', 'VBG'),\n",
    "    (r'.*s$', 'NNS'),\n",
    "]\n",
    "regexp_tag = nltk.RegexpTagger(patterns, backoff = default_tag)\n",
    "unigram_tag = nltk.UnigramTagger(train = [pos_list], backoff = regexp_tag)\n",
    "trump_tag = unigram_tag.tag(word_tokenize(trump))\n",
    "print(trump_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5117f",
   "metadata": {},
   "source": [
    "### In the above code, the UnigramTagger is first invoked to tag the tokens in the Trump article. Whichever words are tagged None by this UnigramTagger are then sent as backoff to the RegexpTagger. The RegexpTagger then tags the words based on the patterns rule it is fed. Any words that are still left untagged are then sent to the DefaultTagger as backoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74d0bd",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6a19c",
   "metadata": {},
   "source": [
    "Let us now deal with the collection of the documents .\n",
    "Collection of the documents is called <b> Corpus <b> . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2ddfcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown \n",
    "\n",
    "# brown corpus is a tagged corpus where each word in each file of the corpus is associated \n",
    "# with the POS tag\n",
    "\n",
    "# Display all the files in the corpus\n",
    "print(brown.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2140d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Standing', 'in', 'the', 'shelter', 'of', 'the', 'tent', '--', 'a', 'rejected', 'hospital', 'tent', 'on', 'which', 'the', 'rain', 'now', 'dripped', ',', 'no', 'longer', 'drumming', '--', 'Adam', 'watched', 'his', 'own', 'hands', 'touch', 'the', 'objects', 'on', 'the', 'improvised', 'counter', 'of', 'boards', 'laid', 'across', 'two', 'beer', 'barrels', '.'], ['There', 'was', ',', 'of', 'course', ',', 'no', 'real', 'need', 'to', 'rearrange', 'everything', '.'], ['A', 'quarter', 'inch', 'this', 'way', 'or', 'that', 'for', 'the', 'hardbake', ',', 'or', 'the', 'toffee', ',', 'or', 'the', 'barley', 'sugar', ',', 'or', 'the', 'sardines', ',', 'or', 'the', 'bitters', ',', 'or', 'the', 'condensed', 'milk', ',', 'or', 'the', 'stationery', ',', 'or', 'the', 'needles', '--', 'what', 'could', 'it', 'mean', '?', '?'], ['Adam', 'watched', 'his', 'own', 'hands', 'make', 'the', 'caressing', ',', 'anxious', 'movement', 'that', ',', 'when', 'rain', 'falls', 'and', 'nobody', 'comes', ',', 'and', 'ruin', 'draws', 'close', 'like', 'a', 'cat', 'rubbing', 'against', 'the', 'ankles', ',', 'has', 'been', 'the', 'ritual', 'of', 'stall', 'vendors', ',', 'forever', '.']], [['He', 'recognized', 'the', 'gesture', '.'], ['He', 'knew', 'its', 'meaning', '.'], ['He', 'had', 'seen', 'a', 'dry', ',', 'old', ',', 'yellowing', 'hand', 'reach', 'out', ',', 'with', 'that', 'painful', 'solicitude', ',', 'to', 'touch', ',', 'to', 'rearrange', ',', 'to', 'shift', 'aimlessly', ',', 'some', 'object', 'worth', 'a', 'pfennig', '.'], ['Back', 'in', 'Bavaria', 'he', 'had', 'seen', 'that', 'gesture', ',', 'and', 'at', 'that', 'sight', 'his', 'heart', 'had', 'always', 'died', 'within', 'him', '.'], ['On', 'such', 'occasions', 'he', 'had', 'not', 'had', 'the', 'courage', 'to', 'look', 'at', 'the', 'face', 'above', 'the', 'hand', ',', 'whatever', 'face', 'it', 'might', 'be', '.']], ...]\n",
      "----------------------------------------\n",
      "[['Standing', 'in', 'the', 'shelter', 'of', 'the', 'tent', '--', 'a', 'rejected', 'hospital', 'tent', 'on', 'which', 'the', 'rain', 'now', 'dripped', ',', 'no', 'longer', 'drumming', '--', 'Adam', 'watched', 'his', 'own', 'hands', 'touch', 'the', 'objects', 'on', 'the', 'improvised', 'counter', 'of', 'boards', 'laid', 'across', 'two', 'beer', 'barrels', '.'], ['There', 'was', ',', 'of', 'course', ',', 'no', 'real', 'need', 'to', 'rearrange', 'everything', '.'], ...]\n",
      "----------------------------------------\n",
      "['Standing', 'in', 'the', 'shelter', 'of', 'the', ...]\n"
     ]
    }
   ],
   "source": [
    "# To display the contents of the file\n",
    "# in the no. of paragraphs, sentences and words\n",
    "print(brown.paras('ck11'))\n",
    "print(\"----------------------------------------\")\n",
    "print(brown.sents('ck11'))\n",
    "print(\"----------------------------------------\")\n",
    "print(brown.words('ck11'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "737da17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Standing', 'VBG'), ('in', 'IN'), ('the', 'AT'), ...]\n"
     ]
    }
   ],
   "source": [
    "# To display yhe POS tag for each word in a specific file of the corpus\n",
    "print(brown.tagged_words('ck11'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5697459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to C:\\Users\\GOD\n",
      "[nltk_data]     WORLD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b0304f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.txt', 'test.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "print(conll2000.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65e1605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Confidence', 'in', 'the', 'pound', 'is', 'widely', ...]\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.words('train.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64d7d860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('S', [Tree('NP', [('Confidence', 'NN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('the', 'DT'), ('pound', 'NN')]), Tree('VP', [('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB')]), Tree('NP', [('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN')]), ('if', 'IN'), Tree('NP', [('trade', 'NN'), ('figures', 'NNS')]), Tree('PP', [('for', 'IN')]), Tree('NP', [('September', 'NNP')]), (',', ','), ('due', 'JJ'), Tree('PP', [('for', 'IN')]), Tree('NP', [('release', 'NN')]), Tree('NP', [('tomorrow', 'NN')]), (',', ','), Tree('VP', [('fail', 'VB'), ('to', 'TO'), ('show', 'VB')]), Tree('NP', [('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN')]), Tree('PP', [('from', 'IN')]), Tree('NP', [('July', 'NNP'), ('and', 'CC'), ('August', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS')]), ('.', '.')]), Tree('S', [('Chancellor', 'NNP'), Tree('PP', [('of', 'IN')]), Tree('NP', [('the', 'DT'), ('Exchequer', 'NNP')]), Tree('NP', [('Nigel', 'NNP'), ('Lawson', 'NNP')]), Tree('NP', [(\"'s\", 'POS'), ('restated', 'VBN'), ('commitment', 'NN')]), Tree('PP', [('to', 'TO')]), Tree('NP', [('a', 'DT'), ('firm', 'NN'), ('monetary', 'JJ'), ('policy', 'NN')]), Tree('VP', [('has', 'VBZ'), ('helped', 'VBN'), ('to', 'TO'), ('prevent', 'VB')]), Tree('NP', [('a', 'DT'), ('freefall', 'NN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('sterling', 'NN')]), Tree('PP', [('over', 'IN')]), Tree('NP', [('the', 'DT'), ('past', 'JJ'), ('week', 'NN')]), ('.', '.')]), ...]\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db3e9e",
   "metadata": {},
   "source": [
    "1. What are the different types of Chunking in NLP?\n",
    "Group of words make up phrases and there are five major categories.\n",
    "  - Noun Phrase (NP)\n",
    "  - Verb phrase (VP)\n",
    "  - Adjective phrase (ADJP)\n",
    "  - Adverb phrase (ADVP)\n",
    "  - Prepositional phrase (PP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c5ddd",
   "metadata": {},
   "source": [
    "# Make our own corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eea189a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barack.txt', 'bush.txt', 'trump.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "path =  \"E:/Natural Language Processing/text_doc/\"\n",
    "president_corpus = PlaintextCorpusReader(path, \".*\")\n",
    "print(president_corpus.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "297c3f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Barack', 'Hussein', 'Obama', 'II', 'born', 'August', '4', ',', '1961', ')', 'is', 'an', 'American', 'politician', 'who', 'served', 'as', 'the', '44th', 'President', 'of', 'the', 'United', 'States', 'from', 'January', '20', ',', '2009', ',', 'to', 'January', '20', ',', '2017', '.'], ['A', 'member', 'of', 'the', 'Democratic', 'Party', ',', 'he', 'was', 'the', 'first', 'African', 'American', 'to', 'assume', 'the', 'presidency', 'and', 'previously', 'served', 'as', 'a', 'United', 'States', 'Senator', 'from', 'Illinois', '(', '2005', '', '2008', ').']]\n"
     ]
    }
   ],
   "source": [
    "# Display the senetences in a sepcifc file\n",
    "print((president_corpus.sents('barack.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbc3a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Barack', 'Hussein', 'Obama', 'II', 'born', 'August', '4', ',', '1961', ')', 'is', 'an', 'American', 'politician', 'who', 'served', 'as', 'the', '44th', 'President', 'of', 'the', 'United', 'States', 'from', 'January', '20', ',', '2009', ',', 'to', 'January', '20', ',', '2017', '.'], ['A', 'member', 'of', 'the', 'Democratic', 'Party', ',', 'he', 'was', 'the', 'first', 'African', 'American', 'to', 'assume', 'the', 'presidency', 'and', 'previously', 'served', 'as', 'a', 'United', 'States', 'Senator', 'from', 'Illinois', '(', '2005', '', '2008', ').'], ...]\n"
     ]
    }
   ],
   "source": [
    "# Display the sentences in all the files of the corpus\n",
    "print(president_corpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c893854c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Hussein', 'Obama', 'II', 'born', 'August', ...]\n"
     ]
    }
   ],
   "source": [
    "# Display the words in as specific file\n",
    "print(president_corpus.words('barack.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff78c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Hussein', 'Obama', 'II', 'born', 'August', ...]\n"
     ]
    }
   ],
   "source": [
    "print(president_corpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743ef8d",
   "metadata": {},
   "source": [
    "Note that the PlainTextCorpusReader automatically splits the text data into paragraphs, sentences and words using appropriate tokenizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c2d9f",
   "metadata": {},
   "source": [
    "# Vectorizing textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ff6e2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "<class 'numpy.ndarray'>\n",
      "   brown  current  dog  fox  india  is  jumps  lazy  minister  modi  narendra  \\\n",
      "0      1        0    1    1      0   0      1     1         0     0         0   \n",
      "1      0        1    0    0      1   1      0     0         1     1         1   \n",
      "\n",
      "   of  over  prime  quick  shri  the  \n",
      "0   0     1      0      1     0    2  \n",
      "1   1     0      1      0     1    1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "sen1= \"India is a republic country. We are proud Indians.\"\n",
    "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
    "count_vectorizer = CountVectorizer()\n",
    "# DTM = document text matrix\n",
    "dtm = count_vectorizer.fit_transform([sent1,sent2])\n",
    "print(type(dtm))\n",
    "print(type(dtm.toarray()))\n",
    "print(pd.DataFrame(data = dtm.toarray(), columns = count_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e0d9b",
   "metadata": {},
   "source": [
    "The similarity between the two documents can now be found using commonly used distance metrics like Euclidean distance or cosine distance.\n",
    "\n",
    "The below code demonstrates the use of cosine distance. The smaller this value, the more similar the two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f0acc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8093074821508816\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "print(cosine(dtm[0].toarray().flat, dtm[1].toarray().flat))\n",
    "# cosine only accepts 1D vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f4702",
   "metadata": {},
   "source": [
    "The values is very high so the two documents are not very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "112eccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30bf8d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        are   country   current     india   indians        is  minister  \\\n",
      "0  0.377628  0.377628  0.000000  0.268685  0.377628  0.268685  0.000000   \n",
      "1  0.000000  0.000000  0.333102  0.237005  0.000000  0.237005  0.333102   \n",
      "\n",
      "       modi  narendra        of     prime     proud  republic      shri  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.377628  0.377628  0.000000   \n",
      "1  0.333102  0.333102  0.333102  0.333102  0.000000  0.000000  0.333102   \n",
      "\n",
      "        the        we  \n",
      "0  0.000000  0.377628  \n",
      "1  0.333102  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "sent1 = \"India is a republic country. We are proud Indians.\"\n",
    "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform([sent1,sent2])\n",
    "print(pd.DataFrame(data = tfidf_vectors.toarray(), columns = tfidf_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcaa2cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8726404702052064\n"
     ]
    }
   ],
   "source": [
    "# Let us calculate the cosine distance\n",
    "from scipy.spatial.distance import cosine\n",
    "print(cosine(tfidf_vectors[0].toarray().flat, tfidf_vectors[1].toarray().flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd455216",
   "metadata": {},
   "source": [
    "Since the values is high, they are not similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1490b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barack.txt', 'bush.txt', 'trump.txt']\n"
     ]
    }
   ],
   "source": [
    "#  Let us now determine the Tf-IDF vectors and similarities between the 3 documents\n",
    "#  in our corpus (president_corpus) \n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "\n",
    "path = \"E:/Natural Language Processing/text_doc/\"\n",
    "president_corpus = PlaintextCorpusReader(path, \".*\", encoding = \"utf-8\")\n",
    "\n",
    "tf_idf = TfidfVectorizer(input='filename')\n",
    "files = [path+filename for filename in list(president_corpus.fileids())]\n",
    "tf_idf_matrix = tf_idf.fit_transform(raw_documents = files)\n",
    "print(president_corpus.fileids())\n",
    "barack = tf_idf_matrix.toarray()[0]\n",
    "bush = tf_idf_matrix.toarray()[1]\n",
    "trump = tf_idf_matrix.toarray()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06aaf082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between articles on barack and bush is:  0.5865048587312349\n",
      "Distance between articles on bush and trump is:  0.4994688378404465\n",
      "Distance between articles on barack and trump is:  0.6510196136918357\n"
     ]
    }
   ],
   "source": [
    "# calculate the cosine distance\n",
    "from scipy.spatial.distance import cosine\n",
    "print(\"Distance between articles on barack and bush is: \",cosine(barack, bush))\n",
    "\n",
    "print(\"Distance between articles on bush and trump is: \", cosine(bush, trump))\n",
    "\n",
    "print(\"Distance between articles on barack and trump is: \", cosine(barack, trump))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c87e3",
   "metadata": {},
   "source": [
    "The most related documents are bush and trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94a59514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between articles on barack and trump is:  1.1410693350465921\n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt\n",
    "def euclidean_distance(x,y):\n",
    "    return sqrt(sum(pow(a-b,2) for a , b in zip(x, y)))\n",
    "print(\"Distance between articles on barack and trump is: \", euclidean_distance(barack, trump))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f11869",
   "metadata": {},
   "source": [
    "# Find about jaccard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1ca29",
   "metadata": {},
   "source": [
    "Jaccard Similarity is used to find similarities between sets. The Jaccard similarity measures similarity between finite sample sets and is defined as the cardinality of the intersection of sets divided by the cardinality of the union of the sample sets.\n",
    "Suppose you want to find Jaccard similarity between two sets A and B, it is the ratio of the cardinality of A  B and A  B.\n",
    "Jaccard Similarity J(A,B) = |AB|/|AB|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cbdcd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between articles on barack and trump is:  0.047619047619047616\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "def jaccard(x, y):\n",
    "    \n",
    "    intersect_card = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_card = len(set.union(*[set(x), set(y)]))\n",
    "    return intersect_card/float(union_card)\n",
    "\n",
    "print(\"Distance between articles on barack and trump is: \", jaccard(barack, trump))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fcbad",
   "metadata": {},
   "source": [
    "If two datasets share the exact same members, their Jaccard Similarity Index will be 1. Conversely, if they have no members in common then their similarity will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607ff84",
   "metadata": {},
   "source": [
    "# Wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a2dff",
   "metadata": {},
   "source": [
    "## detect unusual words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03c37969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knows', 'lol']\n",
      "['nw', 'sms', 'urgnt']\n",
      "['abt']\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "sent1 = \"\"\"Just forced myself to eat a slice. I'm really not hungry tho. \n",
    "           Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\"\"\"\n",
    "sent2 = \"I call you later, don't have nw. If urgnt, sms me.\"\n",
    "sent3 = \"Watching a telugu movie..wat abt u?\"\n",
    "def find_unusual_words(text):\n",
    "    \n",
    "    # finds the vocab set for the words in the sentecnce \n",
    "    # and convert them to lowercase if they are alphabets\n",
    "    text_vocab_set = set(w.lower() for w in text if w.isalpha())\n",
    "    \n",
    "    # make the vocabulary set for the corpus\n",
    "    english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual_set = text_vocab_set - english_vocab_set\n",
    "    return sorted(unusual_set)\n",
    "\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent1)))\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent2)))\n",
    "print(find_unusual_words(nltk.wordpunct_tokenize(sent3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d14fc",
   "metadata": {},
   "source": [
    "## Detect Possible Mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cc18c5",
   "metadata": {},
   "source": [
    "If a word could not found in the word list, it is probable that it is a spelling mistake.\n",
    "\n",
    "The below code, compares the unusual words with known words and suggests possible words based on edit distance. Edit distance is the measure of how similar or dissimilar two words are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7df99ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loa', 'kol', 'col', 'gol', 'lox', 'lod', 'vol', 'lot', 'lo', 'dol', 'lola', 'lop', 'tol', 'lolo', 'log', 'loy', 'loll', 'pol', 'lou', 'sol', 'loo', 'lob', 'lof', 'low']\n"
     ]
    }
   ],
   "source": [
    "unusual_words_found = ['knows', 'lol', 'nw', 'sms', 'urgnt', 'abt']\n",
    "from nltk.metrics import edit_distance\n",
    "possible_suggestions = {}\n",
    "english_vocab_set = set(w.lower() for w in nltk.corpus.words.words())\n",
    "for unusual_word in unusual_words_found:\n",
    "    \n",
    "    for word in english_vocab_set :\n",
    "        \n",
    "        dist = edit_distance(unusual_word, word)\n",
    "        if dist < len(unusual_word)/2:\n",
    "            \n",
    "            if unusual_word not in possible_suggestions.keys():\n",
    "                \n",
    "                possible_suggestions[unusual_word] = [word]\n",
    "            else:\n",
    "                \n",
    "                possible_suggestions[unusual_word].append(word)\n",
    "\n",
    "print(possible_suggestions[\"lol\"])\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917b8fe",
   "metadata": {},
   "source": [
    "# Detect the names of the people "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57f90c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Mary']\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to C:\\Users\\GOD\n",
      "[nltk_data]     WORLD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('names')\n",
    "def names_in_text(text):\n",
    "    \n",
    "    names = []\n",
    "    words_set = set(i for i in text if i.isalpha())\n",
    "    male_names = nltk.corpus.names.words('male.txt')\n",
    "    female_names = nltk.corpus.names.words('female.txt')\n",
    "    for w in words_set:\n",
    "        if male_names.count(w) > 0 or female_names.count(w) > 0:\n",
    "            \n",
    "            names.append(w)\n",
    "\n",
    "    return names\n",
    "\n",
    "sent1 = \"John and Mary go to the church every Sunday\"\n",
    "sent2 = \"No man has ever seen the dark side of the Moon\"\n",
    "print(names_in_text(word_tokenize(sent1)))\n",
    "print(names_in_text(word_tokenize(sent2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8197a775",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88e01316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# To get the all possible meanings of the word \"dog\"\n",
    "print(wn.synsets(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "029230e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'domestic_dog', 'Canis_familiaris']\n"
     ]
    }
   ],
   "source": [
    "# To get the all lemma names of \"dog\"\n",
    "print(wn.synset('dog.n.01').lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e86e3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# A hypernym is the generic term where as a hyponym is a specific ter\n",
    "# like for \"dog\", hypernymns are canine and domestic animal.\n",
    "\n",
    "# Get all hypernyms of the \"dog\"\n",
    "print(wn.synset('dog.n.01').hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b546ba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# Get all the hyponyms of \"dog\"\n",
    "print(wn.synset('dog.n.01').hyponyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d825d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "#Get the path similarity between to words - the method returns the shortest path in the taxonomy\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print(cat.path_similarity(dog)) #Returns a value between 0 and 1. \n",
    "#The higher the number, higher the similarity in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3a84157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goodness', 'goodness', 'commodity', 'trade_good', 'full', 'estimable', 'honorable', 'respectable', 'beneficial', 'just', 'upright', 'adept', 'expert', 'practiced', 'proficient', 'skillful', 'skilful', 'dear', 'near', 'dependable', 'safe', 'secure', 'right', 'ripe', 'well', 'effective', 'in_effect', 'in_force', 'serious', 'sound', 'salutary', 'honest', 'undecomposed', 'unspoiled', 'unspoilt', 'well', 'thoroughly', 'soundly']\n"
     ]
    }
   ],
   "source": [
    "# Get all synonyms of the word 'good'\n",
    "synonyms = []\n",
    "for syn in wn.synsets(\"good\"):\n",
    "    for word in syn.lemmas():\n",
    "        if(word.name() != \"good\"):\n",
    "            synonyms.append(word.name())\n",
    "            \n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e2f5640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "# Get all the antonyms of the word \"good\"\n",
    "\n",
    "antonyms = []\n",
    "for syn in wn.synsets(\"good\"):\n",
    "    for word in syn.lemmas():\n",
    "        if(word.name != 'good' and word.antonyms()):\n",
    "            antonyms.append(word.antonyms()[0].name())\n",
    "            \n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e9515d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "deny\n",
      "abacus\n"
     ]
    }
   ],
   "source": [
    "# Return the base form (morphy) of a word \n",
    "print(wn.morphy(\"working\" , wn.VERB)) #Prints \"work\"\n",
    "print(wn.morphy(\"denied\" , wn.VERB)) #Prints \"deny\"\n",
    "print(wn.morphy(\"abaci\")) #Prints \"abacus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3561e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
